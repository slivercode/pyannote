import os
import json
import argparse
import sys
import time
import pathlib
import shutil
import re

current_script_dir = os.path.dirname(os.path.abspath(__file__))
if current_script_dir not in sys.path:
    sys.path.insert(0, current_script_dir)
print("å­è¿›ç¨‹ sys.pathï¼š", sys.path)

# æ£€æŸ¥æ˜¯å¦æœ‰å¼ºåˆ¶CPUå‚æ•°ï¼ˆéœ€è¦åœ¨å¯¼å…¥torch_loaderä¹‹å‰è®¾ç½®ï¼‰
FORCE_CPU = "--force-cpu" in sys.argv
if FORCE_CPU:
    os.environ["FORCE_CPU_MODE"] = "1"
    print("âš ï¸ æ£€æµ‹åˆ° --force-cpu å‚æ•°ï¼Œå°†å¼ºåˆ¶ä½¿ç”¨CPUè¿è¡Œ")

from torch_loader import use_gpu, torch
from util import (
    get_audio_duration,
    convert_to_wav,
    load_wav,
    extract_media_segment,
)

from pyannote.audio import Pipeline
from pyannote.audio.pipelines.speaker_diarization import DiarizeOutput
import wave
from pyannote.audio.pipelines.utils.hook import ProgressHook
import numpy as np

device = "cuda" if use_gpu else "cpu"
print(f"ä½¿ç”¨è®¾å¤‡: {device}")

EPSILON = 0.1  # æ—¶é—´åŒ¹é…å®¹å·®ï¼ˆç§’ï¼‰
NUM_SPEAKERS = 0  # 0è¡¨ç¤ºè‡ªåŠ¨æ£€æµ‹
MAX_SPEAKERS = 0
MIN_SPEAKERS = 0


def sanitize_filename(filename):
    """
    æ¸…ç†æ–‡ä»¶åä¸­çš„éæ³•å­—ç¬¦ï¼ˆWindows æ–‡ä»¶ç³»ç»Ÿé™åˆ¶ï¼‰
    ç¦ç”¨å­—ç¬¦ï¼š< > : " / \ | ? *
    """
    # Windows æ–‡ä»¶åç¦ç”¨å­—ç¬¦æ˜ å°„è¡¨
    illegal_chars = {
        ':': 'ï¼š',   # å†’å· -> ä¸­æ–‡å†’å·
        '<': 'ï¼œ',   # å°äº -> å…¨è§’å°äº
        '>': 'ï¼',   # å¤§äº -> å…¨è§’å¤§äº
        '"': '"',    # åŒå¼•å· -> ä¸­æ–‡å¼•å·
        '/': 'ï¼',   # æ–œæ  -> å…¨è§’æ–œæ 
        '\\': 'ï¼¼',  # åæ–œæ  -> å…¨è§’åæ–œæ 
        '|': 'ï½œ',   # ç«–çº¿ -> å…¨è§’ç«–çº¿
        '?': 'ï¼Ÿ',   # é—®å· -> ä¸­æ–‡é—®å·
        '*': 'ï¼Š',   # æ˜Ÿå· -> å…¨è§’æ˜Ÿå·
    }
    
    for char, replacement in illegal_chars.items():
        filename = filename.replace(char, replacement)
    
    # ç§»é™¤é¦–å°¾ç©ºæ ¼å’Œç‚¹å·ï¼ˆWindows é™åˆ¶ï¼‰
    filename = filename.strip('. ')
    
    return filename


def get_packaged_cache_path():
    """è·å–ç¼“å­˜è·¯å¾„"""
    if getattr(sys, "frozen", False):
        exe_dir = pathlib.Path(sys.executable).parent
    else:
        exe_dir = pathlib.Path(__file__).parent.parent.parent
    return os.path.abspath(str(exe_dir)).replace(os.sep, "/")


def load_model(auth_token="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", retry=3):
    """åŠ è½½ Pyannote æ¨¡å‹"""
    print(f"=== åˆå§‹åŒ– Pyannote æ¨¡å‹ ===")
    if not auth_token:
        raise ValueError("è¯·æä¾›æœ‰æ•ˆçš„ Hugging Face è®¿é—®ä»¤ç‰Œï¼")

    packaged_cache = get_packaged_cache_path()
    hf_cache = os.path.join(packaged_cache, "hf_cache")
    os.makedirs(hf_cache, exist_ok=True)
    hf_cache = hf_cache.replace(os.sep, "/")
    print(f"Hugging Face ç¼“å­˜ç›®å½•: {hf_cache}")

    for attempt in range(retry):
        try:
            pipeline = Pipeline.from_pretrained(
                "pyannote/speaker-diarization-community-1",
                token=auth_token,
                cache_dir=hf_cache,
            )
            if device == "cuda":
                pipeline.to(torch.device("cuda"))
            print("æ¨¡å‹åŠ è½½æˆåŠŸï¼")
            return pipeline
        except Exception as e:
            if attempt == retry - 1:
                raise RuntimeError(f"æ¨¡å‹åŠ è½½å¤±è´¥ï¼š{str(e)}")
            print(f"âš ï¸  æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œé‡è¯• {attempt+1}/{retry}...")
            time.sleep(5)


class LogHook(ProgressHook):
    """è¿›åº¦é’©å­"""
    def before_pipeline(self, pipeline, **kwargs):
        print(f"å¼€å§‹å¤„ç†éŸ³é¢‘...")

    def before_step(self, step_name, **kwargs):
        print(f"å³å°†æ‰§è¡Œæ­¥éª¤ï¼š{step_name}")

    def update_progress(self, step_name, progress):
        print(f"æ­¥éª¤ {step_name} è¿›åº¦ï¼š{progress*100:.1f}%")

    def after_pipeline(self, pipeline, result, **kwargs):
        print("å¤„ç†å®Œæˆï¼")


def parse_srt_file(srt_path):
    """
    è§£æ SRT å­—å¹•æ–‡ä»¶
    è¿”å›æ ¼å¼: [(start_time, end_time, text), ...]
    æ—¶é—´å•ä½ï¼šç§’
    """
    if not os.path.exists(srt_path):
        raise FileNotFoundError(f"SRTæ–‡ä»¶ä¸å­˜åœ¨ï¼š{srt_path}")

    subtitles = []
    with open(srt_path, "r", encoding="utf-8") as f:
        content = f.read()

    # åˆ†å‰²å­—å¹•å—ï¼ˆä»¥ç©ºè¡Œåˆ†éš”ï¼‰
    blocks = re.split(r'\n\s*\n', content.strip())

    for block in blocks:
        lines = block.strip().split('\n')
        if len(lines) < 3:
            continue

        # è§£ææ—¶é—´è½´ï¼ˆç¬¬äºŒè¡Œï¼‰
        # æ ¼å¼: 00:00:01,000 --> 00:00:03,500
        time_line = lines[1]
        time_match = re.match(
            r'(\d{2}):(\d{2}):(\d{2}),(\d{3})\s*-->\s*(\d{2}):(\d{2}):(\d{2}),(\d{3})',
            time_line
        )

        if not time_match:
            continue

        # è½¬æ¢ä¸ºç§’
        start_h, start_m, start_s, start_ms = map(int, time_match.groups()[:4])
        end_h, end_m, end_s, end_ms = map(int, time_match.groups()[4:])

        start_time = start_h * 3600 + start_m * 60 + start_s + start_ms / 1000
        end_time = end_h * 3600 + end_m * 60 + end_s + end_ms / 1000

        # æå–æ–‡æœ¬ï¼ˆç¬¬ä¸‰è¡ŒåŠä¹‹åï¼‰
        text = '\n'.join(lines[2:]).strip()

        subtitles.append((start_time, end_time, text))

    print(f"âœ… æˆåŠŸè§£æ {len(subtitles)} æ¡å­—å¹•")
    return subtitles


def assign_speakers_to_subtitles(subtitles, diarization_result):
    """
    å°†è¯´è¯äººåˆ†é…ç»™å­—å¹•
    :param subtitles: [(start, end, text), ...]
    :param diarization_result: pyannote çš„åˆ†å‰²ç»“æœ
    :return: [(start, end, text, speaker), ...]
    """
    assigned_subtitles = []

    # æ„å»ºè¯´è¯äººæ—¶é—´æ®µæ˜ å°„
    speaker_segments = []
    for seg in diarization_result.speaker_diarization:
        turn, spk = seg
        speaker_segments.append((turn.start, turn.end, spk))

    for sub_start, sub_end, text in subtitles:
        # æ‰¾åˆ°ä¸å­—å¹•æ—¶é—´é‡å æœ€å¤šçš„è¯´è¯äºº
        best_speaker = "UNKNOWN"
        max_overlap = 0

        for spk_start, spk_end, speaker in speaker_segments:
            # è®¡ç®—é‡å æ—¶é—´
            overlap_start = max(sub_start, spk_start)
            overlap_end = min(sub_end, spk_end)
            overlap_duration = max(0, overlap_end - overlap_start)

            if overlap_duration > max_overlap:
                max_overlap = overlap_duration
                best_speaker = speaker

        # å¦‚æœé‡å æ—¶é—´å¤ªçŸ­ï¼Œæ ‡è®°ä¸ºæœªçŸ¥
        subtitle_duration = sub_end - sub_start
        if max_overlap < subtitle_duration * 0.3:  # è‡³å°‘30%é‡å 
            best_speaker = "UNKNOWN"

        assigned_subtitles.append((sub_start, sub_end, text, best_speaker))

    return assigned_subtitles


def process_video_with_srt(video_path, srt_path, output_path):
    """
    å¤„ç†è§†é¢‘å’ŒSRTæ–‡ä»¶ï¼Œåˆ†é…è¯´è¯äºº
    """
    print(f"PROGRESS:10%")
    
    # è§„èŒƒåŒ–è·¯å¾„ï¼ˆä¿®å¤å°å†™ç›˜ç¬¦ã€æ··åˆæ–œæ ç­‰é—®é¢˜ï¼‰
    video_path = os.path.normpath(video_path)  # å…ˆè§„èŒƒåŒ–
    video_path = os.path.abspath(video_path).replace(os.sep, "/")
    srt_path = os.path.normpath(srt_path)
    srt_path = os.path.abspath(srt_path).replace(os.sep, "/")
    # output_path å¯èƒ½æ˜¯ç›¸å¯¹è·¯å¾„ï¼Œéœ€è¦è½¬æ¢ä¸ºç»å¯¹è·¯å¾„
    output_path = os.path.normpath(output_path)
    if not os.path.isabs(output_path):
        output_path = os.path.abspath(output_path).replace(os.sep, "/")
    else:
        output_path = output_path.replace(os.sep, "/")

    # åˆ›å»ºè¾“å‡ºç›®å½•ï¼ˆæ¸…ç†æ–‡ä»¶åä¸­çš„éæ³•å­—ç¬¦ï¼‰
    base_name = os.path.basename(os.path.normpath(video_path))
    base_name = sanitize_filename(base_name)  # æ¸…ç†éæ³•å­—ç¬¦
    output_dir = os.path.join(output_path, base_name)
    output_dir = output_dir.replace(os.sep, "/")

    if os.path.exists(output_dir):
        shutil.rmtree(output_dir)
    os.makedirs(output_dir, exist_ok=True)

    # åˆ›å»ºä¸´æ—¶ç›®å½•
    tmp_dir = os.path.join(output_dir, "tmp")
    os.makedirs(tmp_dir, exist_ok=True)

    print(f"\nå¤„ç†è§†é¢‘ï¼š{video_path}")
    print(f"å¤„ç†å­—å¹•ï¼š{srt_path}")
    print(f"PROGRESS:20%")

    # æ­¥éª¤1ï¼šè§£æSRTæ–‡ä»¶
    subtitles = parse_srt_file(srt_path)
    print(f"PROGRESS:30%")

    # æ­¥éª¤2ï¼šè½¬ç éŸ³é¢‘
    print("=== è½¬ç éŸ³é¢‘ ===")
    transcode_dir = os.path.join(tmp_dir, "transcoded")
    os.makedirs(transcode_dir, exist_ok=True)
    wav_path = convert_to_wav(video_path, transcode_dir)
    print(f"PROGRESS:40%")

    # æ­¥éª¤3ï¼šåŠ è½½éŸ³é¢‘å¹¶è¿›è¡Œè¯´è¯äººåˆ†å‰²
    print("=== åŠ è½½ Pyannote æ¨¡å‹ ===")
    pipeline = load_model()
    print(f"PROGRESS:50%")

    print("=== æ‰§è¡Œè¯´è¯äººåˆ†å‰² ===")
    input_audio, sr = load_wav(wav_path)
    input_audio_tensor = (
        torch.from_numpy(input_audio.astype(np.float32)).unsqueeze(0).to(device)
    )

    with LogHook() as hook:
        if NUM_SPEAKERS > 0:
            diarize_result: DiarizeOutput = pipeline(
                {"waveform": input_audio_tensor, "sample_rate": sr},
                hook=hook,
                num_speakers=NUM_SPEAKERS,
            )
        elif MIN_SPEAKERS > 0 and MAX_SPEAKERS > 0:
            diarize_result: DiarizeOutput = pipeline(
                {"waveform": input_audio_tensor, "sample_rate": sr},
                hook=hook,
                min_speakers=MIN_SPEAKERS,
                max_speakers=MAX_SPEAKERS,
            )
        else:
            diarize_result: DiarizeOutput = pipeline(
                {"waveform": input_audio_tensor, "sample_rate": sr}, hook=hook
            )

    speakers = list(diarize_result.speaker_diarization.labels())
    print(f"  â†’ å…±è¯†åˆ«å‡º {len(speakers)} ä¸ªè¯´è¯äºº")
    print(f"count_roleï¼š{len(speakers)}")
    print(f"PROGRESS:70%")

    # æ­¥éª¤4ï¼šåˆ†é…è¯´è¯äººåˆ°å­—å¹•
    print("=== åˆ†é…è¯´è¯äººåˆ°å­—å¹• ===")
    assigned_subtitles = assign_speakers_to_subtitles(subtitles, diarize_result)

    # æ­¥éª¤5ï¼šæŒ‰è¯´è¯äººåˆ†ç»„å­—å¹•
    speaker_subtitles = {}
    for start, end, text, speaker in assigned_subtitles:
        speaker_id = speaker.replace("SPEAKER_", "spk")
        if speaker_id not in speaker_subtitles:
            speaker_subtitles[speaker_id] = []
        speaker_subtitles[speaker_id].append({
            "å¼€å§‹æ—¶é—´(ç§’)": round(start, 2),
            "ç»“æŸæ—¶é—´(ç§’)": round(end, 2),
            "æŒç»­æ—¶é—´(ç§’)": round(end - start, 2),
            "æ–‡æœ¬å†…å®¹": text
        })

    print(f"PROGRESS:80%")

    # æ­¥éª¤6ï¼šä¿å­˜ç»“æœ
    # ä¿å­˜å®Œæ•´çš„å­—å¹•åˆ†é…ç»“æœ
    all_results = []
    for start, end, text, speaker in assigned_subtitles:
        speaker_id = speaker.replace("SPEAKER_", "spk")
        all_results.append({
            "å¼€å§‹æ—¶é—´(ç§’)": round(start, 2),
            "ç»“æŸæ—¶é—´(ç§’)": round(end, 2),
            "æŒç»­æ—¶é—´(ç§’)": round(end - start, 2),
            "è¯´è¯äºº": speaker_id,
            "æ–‡æœ¬å†…å®¹": text
        })

    result_path = os.path.join(output_dir, "å­—å¹•è¯´è¯äººåˆ†é…ç»“æœ.json")
    with open(result_path, "w", encoding="utf-8") as f:
        json.dump(all_results, f, ensure_ascii=False, indent=2)
    print(f"âœ… å®Œæ•´ç»“æœå·²ä¿å­˜ï¼š{result_path}")

    # è°ƒç”¨ tosrt2.py ç”Ÿæˆåˆå¹¶çš„ SRT æ–‡ä»¶
    try:
        # å¯¼å…¥ tosrt2.py çš„è½¬æ¢å‡½æ•°
        tosrt2_path = os.path.join(pathlib.Path(__file__).parent.parent.parent, "tosrt", "tosrt2.py")
        if os.path.exists(tosrt2_path):
            # åŠ¨æ€å¯¼å…¥ tosrt2 æ¨¡å—
            import importlib.util
            spec = importlib.util.spec_from_file_location("tosrt2", tosrt2_path)
            tosrt2 = importlib.util.module_from_spec(spec)
            spec.loader.exec_module(tosrt2)
            
            # ç”Ÿæˆåˆå¹¶çš„ SRT æ–‡ä»¶ï¼ˆåŒ…å«è¯´è¯äººæ ‡è¯†ï¼‰
            print(f"\nğŸ“ æ­£åœ¨ç”Ÿæˆåˆå¹¶çš„ SRT æ–‡ä»¶...")
            tosrt2.convert_json_to_srt(result_path, no_speaker=False)
            
            # ç”Ÿæˆè¾“å‡ºè·¯å¾„
            merged_srt_path = os.path.splitext(result_path)[0] + ".srt"
            print(f"âœ… åˆå¹¶ SRT æ–‡ä»¶å·²ä¿å­˜ï¼š{merged_srt_path}")
            print(f"result_merged_srtï¼š{merged_srt_path}")
        else:
            print(f"âš ï¸ è­¦å‘Šï¼šæœªæ‰¾åˆ° tosrt2.pyï¼Œè·³è¿‡åˆå¹¶ SRT ç”Ÿæˆ")
    except Exception as e:
        print(f"âš ï¸ ç”Ÿæˆåˆå¹¶ SRT æ–‡ä»¶æ—¶å‡ºé”™ï¼š{e}")

    # æŒ‰è¯´è¯äººä¿å­˜åˆ†ç»„ç»“æœ
    speaker_dir = os.path.join(output_dir, "speaker_subtitles")
    os.makedirs(speaker_dir, exist_ok=True)

    for speaker_id, subs in speaker_subtitles.items():
        # ä¿å­˜JSONæ ¼å¼
        speaker_json_path = os.path.join(speaker_dir, f"{speaker_id}_å­—å¹•.json")
        with open(speaker_json_path, "w", encoding="utf-8") as f:
            json.dump(subs, f, ensure_ascii=False, indent=2)
        print(f"âœ… {speaker_id} å­—å¹•å·²ä¿å­˜ï¼š{speaker_json_path}")

        # ä¿å­˜SRTæ ¼å¼
        speaker_srt_path = os.path.join(speaker_dir, f"{speaker_id}_å­—å¹•.srt")
        with open(speaker_srt_path, "w", encoding="utf-8") as f:
            for idx, sub in enumerate(subs, 1):
                start_sec = sub["å¼€å§‹æ—¶é—´(ç§’)"]
                end_sec = sub["ç»“æŸæ—¶é—´(ç§’)"]
                
                # è½¬æ¢ä¸ºSRTæ—¶é—´æ ¼å¼
                start_h = int(start_sec // 3600)
                start_m = int((start_sec % 3600) // 60)
                start_s = int(start_sec % 60)
                start_ms = int((start_sec % 1) * 1000)
                
                end_h = int(end_sec // 3600)
                end_m = int((end_sec % 3600) // 60)
                end_s = int(end_sec % 60)
                end_ms = int((end_sec % 1) * 1000)
                
                f.write(f"{idx}\n")
                f.write(f"{start_h:02d}:{start_m:02d}:{start_s:02d},{start_ms:03d} --> ")
                f.write(f"{end_h:02d}:{end_m:02d}:{end_s:02d},{end_ms:03d}\n")
                f.write(f"{sub['æ–‡æœ¬å†…å®¹']}\n\n")
        
        print(f"âœ… {speaker_id} SRTå­—å¹•å·²ä¿å­˜ï¼š{speaker_srt_path}")
        print(f"result_speaker_srtï¼š{speaker_srt_path}")

    print(f"PROGRESS:90%")
    print(f"result_rootï¼š{output_dir}")
    print(f"PROGRESS:100%")

    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
    # shutil.rmtree(tmp_dir)


if __name__ == "__main__":
    print(f"PROGRESS:5%")
    parser = argparse.ArgumentParser(description="è§†é¢‘+OCRå­—å¹•è¯´è¯äººåˆ†é…")
    parser.add_argument(
        "--video-path",
        type=str,
        required=True,
        help="è§†é¢‘æ–‡ä»¶è·¯å¾„ï¼ˆMP4ç­‰æ ¼å¼ï¼‰",
    )
    parser.add_argument(
        "--srt-path",
        type=str,
        required=True,
        help="SRTå­—å¹•æ–‡ä»¶è·¯å¾„",
    )
    parser.add_argument(
        "--output-path",
        type=str,
        default="output",
        help="è¾“å‡ºæ ¹ç›®å½•ï¼ˆé»˜è®¤å½“å‰ç›®å½•ä¸‹çš„ outputï¼‰",
    )
    parser.add_argument(
        "--num-speakers",
        type=int,
        default=0,
        help="è§’è‰²æ•°é‡ï¼ˆ0ä¸ºè‡ªåŠ¨æ£€æµ‹ï¼‰",
    )
    parser.add_argument(
        "--min-speakers",
        type=int,
        default=0,
        help="è§’è‰²æ•°é‡èŒƒå›´æœ€å°å€¼",
    )
    parser.add_argument(
        "--max-speakers",
        type=int,
        default=0,
        help="è§’è‰²æ•°é‡èŒƒå›´æœ€å¤§å€¼",
    )
    parser.add_argument(
        "--force-cpu",
        action="store_true",
        help="å¼ºåˆ¶ä½¿ç”¨CPUè¿è¡Œï¼ˆå¿½ç•¥GPUï¼‰",
    )

    args = parser.parse_args()

    try:
        NUM_SPEAKERS = args.num_speakers
        MIN_SPEAKERS = args.min_speakers
        MAX_SPEAKERS = args.max_speakers

        start_time = time.time()
        process_video_with_srt(
            video_path=args.video_path,
            srt_path=args.srt_path,
            output_path=args.output_path,
        )

        total_time = (time.time() - start_time) / 60
        print(f"\nâ±ï¸  æ€»è€—æ—¶ï¼š{total_time:.2f} åˆ†é’Ÿ")

    except Exception as e:
        print(f"\nâŒ æ‰§è¡Œå¤±è´¥ï¼š{str(e)}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
