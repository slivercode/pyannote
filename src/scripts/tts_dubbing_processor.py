"""
TTSé…éŸ³å¤„ç†å™¨
è§£æSRTå­—å¹•æ–‡ä»¶ï¼Œè°ƒç”¨TTS APIç”Ÿæˆè¯­éŸ³ï¼ŒæŒ‰æ—¶é—´è½´æ‹¼æ¥éŸ³é¢‘
æ”¯æŒåŒé‡å˜é€Ÿæœºåˆ¶ï¼šæ™ºèƒ½éŸ³é¢‘åŠ é€Ÿå’Œè§†é¢‘æ…¢é€Ÿ
"""

import os
import re
import requests
from pathlib import Path
from pydub import AudioSegment
from pydub.generators import Sine
import time
from speed_rate_adjuster import SpeedRateAdjuster
from timeline_adjuster import TimelineAdjuster


class TTSDubbingProcessor:
    def __init__(self, srt_path, output_dir, engine, role_data, text_lang='zh',
                 speed_factor=1.0, silence_duration=0.5, auto_align=True, 
                 api_url=None, api_key=None, task_id=None, task_dict=None,
                 enable_smart_speedup=False, enable_audio_speedup=True, 
                 enable_video_slowdown=False, max_audio_speed_rate=2.0,
                 max_video_pts_rate=10.0, remove_silent_gaps=False,
                 preserve_total_time=False):  # é»˜è®¤ä¸ä¿æŒæ€»æ—¶é•¿ï¼Œä¿æŒåŸå§‹é—´éš”
        """
        åˆå§‹åŒ–TTSé…éŸ³å¤„ç†å™¨
        
        Args:
            srt_path: SRTå­—å¹•æ–‡ä»¶è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•
            engine: TTSå¼•æ“ ('gpt-sovits' æˆ– 'qwen-tts')
            role_data: è§’è‰²é…ç½®æ•°æ®
            text_lang: åˆæˆè¯­è¨€ ('zh', 'en', 'ja', 'ko')
            speed_factor: è¯­é€Ÿç³»æ•°
            silence_duration: é™éŸ³é—´éš”æ—¶é•¿(ç§’)
            auto_align: æ˜¯å¦è‡ªåŠ¨å¯¹é½æ—¶é—´è½´
            api_url: GPT-SoVITS APIåœ°å€
            api_key: QwenTTS APIå¯†é’¥
            task_id: ä»»åŠ¡ID
            task_dict: ä»»åŠ¡çŠ¶æ€å­—å…¸
            enable_smart_speedup: æ˜¯å¦å¯ç”¨æ™ºèƒ½åŒé‡å˜é€Ÿæœºåˆ¶
            enable_audio_speedup: æ˜¯å¦å¯ç”¨éŸ³é¢‘åŠ é€Ÿ
            enable_video_slowdown: æ˜¯å¦å¯ç”¨è§†é¢‘æ…¢é€Ÿ
            max_audio_speed_rate: éŸ³é¢‘æœ€å¤§åŠ é€Ÿå€ç‡
            max_video_pts_rate: è§†é¢‘æœ€å¤§æ…¢é€Ÿå€ç‡
            remove_silent_gaps: æ˜¯å¦ç§»é™¤å­—å¹•é—´é™éŸ³é—´éš™
            preserve_total_time: æ˜¯å¦ä¿æŒSRTæ€»æ—¶é•¿ä¸å˜ï¼ˆåŠ¨æ€è°ƒæ•´æ—¶é—´è½´ï¼‰
        """
        self.srt_path = srt_path
        self.output_dir = Path(output_dir)
        self.engine = engine
        self.role_data = role_data
        self.text_lang = text_lang  # æ–°å¢ï¼šåˆæˆè¯­è¨€
        self.speed_factor = speed_factor
        self.silence_duration = silence_duration
        self.auto_align = auto_align
        self.api_url = api_url
        self.api_key = api_key
        self.task_id = task_id
        self.task_dict = task_dict
        
        # åŒé‡å˜é€Ÿæœºåˆ¶å‚æ•°
        self.enable_smart_speedup = enable_smart_speedup
        self.enable_audio_speedup = enable_audio_speedup
        self.enable_video_slowdown = enable_video_slowdown
        self.max_audio_speed_rate = max_audio_speed_rate
        self.max_video_pts_rate = max_video_pts_rate
        self.remove_silent_gaps = remove_silent_gaps
        self.preserve_total_time = preserve_total_time
        
        # åˆ›å»ºä¸´æ—¶ç›®å½•
        self.temp_dir = self.output_dir / "temp_audio"
        self.temp_dir.mkdir(parents=True, exist_ok=True)
        
    def parse_srt(self):
        """è§£æSRTæ–‡ä»¶ï¼ˆæ”¯æŒè¯´è¯äººæ ‡è®°ï¼‰"""
        print(f"ğŸ“– å¼€å§‹è§£æSRTæ–‡ä»¶: {self.srt_path}")
        
        with open(self.srt_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        print(f"ğŸ“„ æ–‡ä»¶å¤§å°: {len(content)} å­—ç¬¦")
        print(f"ğŸ“„ æ–‡ä»¶å‰200å­—ç¬¦: {repr(content[:200])}")
        
        subtitles = []
        # è§„èŒƒåŒ–æ¢è¡Œç¬¦
        content_normalized = content.replace('\r\n', '\n').replace('\r', '\n')
        
        # å¤„ç†ä¸¤ç§æ ¼å¼ï¼š
        # 1. æ ‡å‡†æ ¼å¼ï¼šè¡Œé—´å•æ¢è¡Œï¼Œå—é—´åŒæ¢è¡Œ
        # 2. éæ ‡å‡€æ ¼å¼ï¼šè¡Œé—´åŒæ¢è¡Œï¼Œå—é—´å¤šä¸ªæ¢è¡Œ
        # å…ˆå°è¯•æ ‡å‡†æ ¼å¼
        blocks = re.split(r'\n\n+', content_normalized.strip())
        
        # å¦‚æœæ‰€æœ‰å—éƒ½åªæœ‰1è¡Œï¼Œè¯´æ˜æ˜¯éæ ‡å‡†æ ¼å¼ï¼Œéœ€è¦é‡æ–°åˆ†ç»„
        if all(len(block.strip().split('\n')) == 1 for block in blocks if block.strip()):
            print("âš ï¸ æ£€æµ‹åˆ°éæ ‡å‡†SRTæ ¼å¼ï¼ˆè¡Œé—´åŒæ¢è¡Œï¼‰ï¼Œé‡æ–°åˆ†ç»„...")
            # æ¯3è¡Œä¸ºä¸€ç»„ï¼ˆåºå·ã€æ—¶é—´ã€æ–‡æœ¬ï¼‰
            lines = [line for line in content_normalized.strip().split('\n') if line.strip()]
            blocks = []
            for i in range(0, len(lines), 3):
                if i + 2 < len(lines):
                    blocks.append('\n'.join(lines[i:i+3]))
                elif i < len(lines):
                    # å¤„ç†æœ€åä¸å®Œæ•´çš„å—
                    blocks.append('\n'.join(lines[i:]))
        
        print(f"ğŸ“¦ åˆ†å‰²åçš„å—æ•°: {len(blocks)}")
        
        for i, block in enumerate(blocks):
            block = block.strip()
            if not block:
                continue
                
            lines = block.split('\n')
            print(f"ğŸ” å— {i+1}: {len(lines)} è¡Œ - {lines[0] if lines else 'empty'}")
            
            if len(lines) >= 3:
                # è§£ææ—¶é—´è½´
                time_match = re.match(r'(\d{2}:\d{2}:\d{2},\d{3})\s*-->\s*(\d{2}:\d{2}:\d{2},\d{3})', lines[1])
                if time_match:
                    text_content = ' '.join(lines[2:])
                    
                    # æå–è¯´è¯äººä¿¡æ¯ï¼ˆæ”¯æŒå¤šç§æ ¼å¼ï¼‰
                    speaker = None
                    clean_text = text_content
                    
                    # åŒ¹é… [spkXX] æ ¼å¼ï¼ˆå¤šè§’è‰²é…éŸ³ï¼‰
                    speaker_match = re.match(r'\[(spk\d+)\]\s*(.*)', text_content)
                    if speaker_match:
                        speaker = speaker_match.group(1)
                        clean_text = speaker_match.group(2)
                    else:
                        # åŒ¹é… [SPEAKER_XX] æ ¼å¼
                        speaker_match = re.match(r'\[(SPEAKER_\d+)\]\s*(.*)', text_content)
                        if speaker_match:
                            speaker = speaker_match.group(1)
                            clean_text = speaker_match.group(2)
                        else:
                            # åŒ¹é… spkXX: æ ¼å¼
                            speaker_match = re.match(r'(spk\d+):\s*(.*)', text_content)
                            if speaker_match:
                                speaker = speaker_match.group(1)
                                clean_text = speaker_match.group(2)
                            else:
                                # åŒ¹é… SPEAKER_XX: æ ¼å¼
                                speaker_match = re.match(r'(SPEAKER_\d+):\s*(.*)', text_content)
                                if speaker_match:
                                    speaker = speaker_match.group(1)
                                    clean_text = speaker_match.group(2)
                    
                    subtitle = {
                        'index': int(lines[0]),
                        'start': time_match.group(1),
                        'end': time_match.group(2),
                        'text': clean_text.strip(),
                        'speaker': speaker  # æ·»åŠ è¯´è¯äººä¿¡æ¯
                    }
                    subtitles.append(subtitle)
                    
                    if speaker:
                        print(f"âœ… è§£æå­—å¹• {subtitle['index']} [{speaker}]: {clean_text[:30]}...")
                    else:
                        print(f"âœ… è§£æå­—å¹• {subtitle['index']}: {clean_text[:30]}...")
                else:
                    print(f"âš ï¸ å— {i+1} æ—¶é—´æ ¼å¼ä¸åŒ¹é…: {lines[1] if len(lines) > 1 else 'N/A'}")
            else:
                print(f"âš ï¸ å— {i+1} è¡Œæ•°ä¸è¶³ ({len(lines)} < 3)")
        
        print(f"âœ… è§£æSRTæ–‡ä»¶æˆåŠŸï¼Œå…± {len(subtitles)} æ¡å­—å¹•")
        return subtitles
    
    def time_to_ms(self, time_str):
        """å°†SRTæ—¶é—´æ ¼å¼è½¬æ¢ä¸ºæ¯«ç§’"""
        # æ ¼å¼: 00:00:05,500
        h, m, s_ms = time_str.split(':')
        s, ms = s_ms.split(',')
        total_ms = int(h) * 3600000 + int(m) * 60000 + int(s) * 1000 + int(ms)
        return total_ms
    
    def synthesize_speech(self, text, index, speaker=None, target_duration_ms=None):
        """
        è°ƒç”¨TTS APIåˆæˆè¯­éŸ³
        
        Args:
            text: è¦åˆæˆçš„æ–‡æœ¬
            index: å­—å¹•ç´¢å¼•
            speaker: è¯´è¯äººæ ‡è¯†ï¼ˆå¯é€‰ï¼‰
            target_duration_ms: ç›®æ ‡æ—¶é•¿ï¼ˆæ¯«ç§’ï¼‰ï¼Œå¦‚æœæä¾›åˆ™è‡ªåŠ¨è°ƒæ•´è¯­é€Ÿ
            
        Returns:
            éŸ³é¢‘æ–‡ä»¶è·¯å¾„
        """
        output_path = self.temp_dir / f"audio_{index:04d}.wav"
        
        try:
            if self.engine == 'gpt-sovits':
                return self._synthesize_gpt_sovits(text, output_path, speaker, target_duration_ms)
            elif self.engine == 'qwen-tts':
                return self._synthesize_qwen_tts(text, output_path, speaker, target_duration_ms)
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„TTSå¼•æ“: {self.engine}")
        except Exception as e:
            print(f"âŒ åˆæˆè¯­éŸ³å¤±è´¥ (å­—å¹•{index}): {e}")
            raise
    
    def _synthesize_gpt_sovits(self, text, output_path, speaker=None, target_duration_ms=None):
        """ä½¿ç”¨GPT-SoVITSåˆæˆè¯­éŸ³ï¼ˆæ”¯æŒå¤šè§’è‰²å’Œè‡ªåŠ¨è¯­é€Ÿè°ƒæ•´ï¼‰"""
        # ç¡®ä¿APIåœ°å€æ­£ç¡®
        api_url = self.api_url
        if not api_url.endswith('/tts'):
            api_url += '/tts'
        
        # æ ¹æ®è¯´è¯äººé€‰æ‹©è§’è‰²é…ç½®
        role_config = None
        if speaker and isinstance(self.role_data, dict) and speaker in self.role_data:
            # å¤šè§’è‰²æ¨¡å¼ï¼šä»role_dataä¸­è·å–å¯¹åº”è§’è‰²çš„é…ç½®
            role_config = self.role_data[speaker]
            print(f"ğŸ­ ä½¿ç”¨è§’è‰²é…ç½®: {speaker}")
        elif isinstance(self.role_data, dict) and 'refAudioPath' in self.role_data:
            # å•è§’è‰²æ¨¡å¼ï¼šç›´æ¥ä½¿ç”¨role_data
            role_config = self.role_data
        else:
            # å…œåº•ï¼šå°è¯•ä½¿ç”¨é»˜è®¤é…ç½®
            if isinstance(self.role_data, dict) and 'default' in self.role_data:
                role_config = self.role_data['default']
                print(f"âš ï¸ æœªæ‰¾åˆ°è§’è‰² {speaker} çš„é…ç½®ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
            else:
                # å®Œå…¨æ²¡æœ‰é…ç½®ï¼Œç”Ÿæˆé™éŸ³éŸ³é¢‘ä½œä¸ºå ä½
                print(f"âŒ æœªæ‰¾åˆ°è§’è‰² {speaker} çš„é…ç½®ï¼Œä¸”æ— é»˜è®¤é…ç½®")
                print(f"   ç”Ÿæˆé™éŸ³éŸ³é¢‘ä½œä¸ºå ä½ï¼ˆæ—¶é•¿: {target_duration_ms or 1000}msï¼‰")
                
                # ç”Ÿæˆé™éŸ³éŸ³é¢‘
                duration_ms = target_duration_ms if target_duration_ms else 1000
                silence = AudioSegment.silent(duration=duration_ms)
                silence.export(output_path, format="wav")
                
                return str(output_path)
        
        # éªŒè¯å¿…è¦å­—æ®µ
        if not role_config.get('refAudioPath'):
            print(f"âš ï¸ è§’è‰² {speaker} ç¼ºå°‘å‚è€ƒéŸ³é¢‘è·¯å¾„ï¼Œç”Ÿæˆé™éŸ³å ä½")
            duration_ms = target_duration_ms if target_duration_ms else 1000
            silence = AudioSegment.silent(duration=duration_ms)
            silence.export(output_path, format="wav")
            return str(output_path)
        
        # è·å–è¯¥è§’è‰²çš„è¯­é€Ÿç³»æ•°ï¼ˆä¼˜å…ˆä½¿ç”¨è§’è‰²é…ç½®ï¼Œå¦åˆ™ä½¿ç”¨å…¨å±€é…ç½®ï¼‰
        role_speed_factor = role_config.get('speed_factor', self.speed_factor)
        
        # å¦‚æœæä¾›äº†ç›®æ ‡æ—¶é•¿ï¼Œå…ˆç”¨æ ‡å‡†è¯­é€Ÿç”Ÿæˆä¸€æ¬¡ï¼Œæµ‹é‡å®é™…æ—¶é•¿ï¼Œç„¶åè®¡ç®—éœ€è¦çš„è¯­é€Ÿ
        if target_duration_ms and self.auto_align:
            # ç¬¬ä¸€æ¬¡ï¼šç”¨æ ‡å‡†è¯­é€Ÿç”Ÿæˆï¼Œæµ‹é‡æ—¶é•¿
            temp_output = self.temp_dir / f"temp_{output_path.name}"
            
            # è·å–ç›®æ ‡è¯­è¨€
            target_lang_test = role_config.get('text_lang', self.text_lang)
            
            params_test = {
                'text': text,
                'text_lang': target_lang_test,
                'ref_audio_path': role_config.get('refAudioPath', ''),
                'prompt_text': role_config.get('promptText', ''),
                'prompt_lang': role_config.get('promptLang', target_lang_test),  # ä½¿ç”¨ç›®æ ‡è¯­è¨€ä½œä¸ºé»˜è®¤å€¼
                'speed_factor': 1.0  # å…ˆç”¨æ ‡å‡†è¯­é€Ÿæµ‹è¯•
            }
            
            response_test = requests.get(api_url, params=params_test, timeout=60)
            response_test.raise_for_status()
            
            with open(temp_output, 'wb') as f:
                f.write(response_test.content)
            
            # æµ‹é‡å®é™…æ—¶é•¿
            audio_test = AudioSegment.from_file(str(temp_output))
            actual_duration_ms = len(audio_test)
            
            # è®¡ç®—éœ€è¦çš„è¯­é€Ÿï¼ˆé™åˆ¶åœ¨åˆç†èŒƒå›´å†…ï¼‰
            required_speed = actual_duration_ms / target_duration_ms
            
            # é™åˆ¶è¯­é€ŸèŒƒå›´ï¼ˆ0.5x - 2.0xï¼‰
            required_speed = max(0.5, min(2.0, required_speed))
            
            tts_speed_factor = required_speed
            
            print(f"  ğŸ“Š æµ‹è¯•æ—¶é•¿: {actual_duration_ms}ms, ç›®æ ‡: {target_duration_ms}ms, è®¡ç®—è¯­é€Ÿ: {required_speed:.2f}x")
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            try:
                os.unlink(temp_output)
            except:
                pass
        else:
            # ç›´æ¥ä½¿ç”¨è®¾å®šçš„è¯­é€Ÿ
            tts_speed_factor = role_speed_factor
        
        # è·å–ç›®æ ‡è¯­è¨€
        target_lang = role_config.get('text_lang', self.text_lang)
        
        # æ™ºèƒ½è·å–å‚è€ƒæ–‡æœ¬è¯­è¨€
        # ä¼˜å…ˆä½¿ç”¨é…ç½®ä¸­çš„promptLangï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨ç›®æ ‡è¯­è¨€
        prompt_lang = role_config.get('promptLang', target_lang)
        
        # æ„å»ºè¯·æ±‚å‚æ•°
        params = {
            'text': text,
            'text_lang': target_lang,
            'ref_audio_path': role_config.get('refAudioPath', ''),
            'prompt_text': role_config.get('promptText', ''),
            'prompt_lang': prompt_lang,  # ä½¿ç”¨æ™ºèƒ½è·å–çš„è¯­è¨€
            'speed_factor': tts_speed_factor  # ä½¿ç”¨è®¡ç®—åçš„è¯­é€Ÿ
        }
        
        if speaker:
            print(f"ğŸ”„ è°ƒç”¨GPT-SoVITS API [{speaker}, è¯­é€Ÿ={tts_speed_factor:.2f}x]: {text[:30]}...")
        else:
            print(f"ğŸ”„ è°ƒç”¨GPT-SoVITS API [è¯­é€Ÿ={tts_speed_factor:.2f}x]: {text[:30]}...")
        
        # è¾“å‡ºè¯¦ç»†å‚æ•°ï¼ˆç”¨äºè¯Šæ–­ï¼‰
        print(f"   ç›®æ ‡è¯­è¨€: {target_lang}")
        print(f"   å‚è€ƒæ–‡æœ¬: {params['prompt_text'][:30]}...")
        print(f"   å‚è€ƒè¯­è¨€: {prompt_lang}")  # é‡ç‚¹ï¼šæ£€æŸ¥è¿™ä¸ªæ˜¯å¦æ­£ç¡®
        
        # å‘é€è¯·æ±‚
        response = requests.get(api_url, params=params, timeout=60)
        response.raise_for_status()
        
        # ä¿å­˜éŸ³é¢‘
        with open(output_path, 'wb') as f:
            f.write(response.content)
        
        # éªŒè¯æœ€ç»ˆæ—¶é•¿
        if target_duration_ms:
            audio_final = AudioSegment.from_file(str(output_path))
            final_duration = len(audio_final)
            print(f"âœ… è¯­éŸ³åˆæˆæˆåŠŸ: {output_path.name}, æ—¶é•¿: {final_duration}ms (ç›®æ ‡: {target_duration_ms}ms)")
        else:
            print(f"âœ… è¯­éŸ³åˆæˆæˆåŠŸ: {output_path.name}")
        
        return str(output_path)
    
    def _synthesize_qwen_tts(self, text, output_path, speaker=None, target_duration_ms=None):
        """ä½¿ç”¨QwenTTSåˆæˆè¯­éŸ³ï¼ˆæ”¯æŒå¤šè§’è‰²ï¼‰"""
        import dashscope
        from dashscope.audio.tts import SpeechSynthesizer
        
        # è®¾ç½®APIå¯†é’¥
        dashscope.api_key = self.api_key
        
        # æ ¹æ®è¯´è¯äººé€‰æ‹©è§’è‰²é…ç½®
        role_config = None
        if speaker and isinstance(self.role_data, dict) and speaker in self.role_data:
            # å¤šè§’è‰²æ¨¡å¼ï¼šä»role_dataä¸­è·å–å¯¹åº”è§’è‰²çš„é…ç½®
            role_config = self.role_data[speaker]
            print(f"ğŸ­ ä½¿ç”¨è§’è‰²é…ç½®: {speaker}")
        elif isinstance(self.role_data, dict) and 'voice' in self.role_data:
            # å•è§’è‰²æ¨¡å¼ï¼šç›´æ¥ä½¿ç”¨role_data
            role_config = self.role_data
        else:
            # å…œåº•ï¼šå°è¯•ä½¿ç”¨é»˜è®¤é…ç½®
            if isinstance(self.role_data, dict) and 'default' in self.role_data:
                role_config = self.role_data['default']
                print(f"âš ï¸ æœªæ‰¾åˆ°è§’è‰² {speaker} çš„é…ç½®ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
            else:
                # å®Œå…¨æ²¡æœ‰é…ç½®ï¼Œç”Ÿæˆé™éŸ³éŸ³é¢‘ä½œä¸ºå ä½
                print(f"âŒ æœªæ‰¾åˆ°è§’è‰² {speaker} çš„é…ç½®ï¼Œä¸”æ— é»˜è®¤é…ç½®")
                print(f"   ç”Ÿæˆé™éŸ³éŸ³é¢‘ä½œä¸ºå ä½ï¼ˆæ—¶é•¿: {target_duration_ms or 1000}msï¼‰")
                
                # ç”Ÿæˆé™éŸ³éŸ³é¢‘
                duration_ms = target_duration_ms if target_duration_ms else 1000
                silence = AudioSegment.silent(duration=duration_ms)
                silence.export(output_path, format="wav")
                
                return str(output_path)
        
        # è·å–è¯¥è§’è‰²çš„è¯­é€Ÿç³»æ•°ï¼ˆä¼˜å…ˆä½¿ç”¨è§’è‰²é…ç½®ï¼Œå¦åˆ™ä½¿ç”¨å…¨å±€é…ç½®ï¼‰
        role_speed_factor = role_config.get('speed_factor', self.speed_factor)
        
        # è·å–å£°éŸ³é…ç½®
        voice = role_config.get('voice', 'å¢¨è®²å¸ˆ')  # é»˜è®¤ä½¿ç”¨å¢¨è®²å¸ˆ
        
        # æ ¹æ®æ–‡æœ¬è¯­è¨€é€‰æ‹©åˆé€‚çš„æ¨¡å‹
        model = self._select_qwen_model(text, role_config)
        
        # æ„å»ºè¯·æ±‚å‚æ•°
        params = {
            'model': model,
            'text': text,
            'sample_rate': 48000
        }
        
        # å¦‚æœæŒ‡å®šäº†å£°éŸ³ï¼Œå°è¯•ä½¿ç”¨ï¼ˆæŸäº›æ¨¡å‹æ”¯æŒï¼‰
        if voice and voice != 'default':
            # å¯¹äºæ”¯æŒå¤šå£°éŸ³çš„æ¨¡å‹ï¼Œå¯ä»¥æ·»åŠ voiceå‚æ•°
            # params['voice'] = voice
            pass
        
        if speaker:
            print(f"ğŸ”„ è°ƒç”¨Qwen TTS API [{speaker}, å£°éŸ³={voice}, è¯­é€Ÿ={role_speed_factor}]: {text[:30]}...")
        else:
            print(f"ğŸ”„ è°ƒç”¨Qwen TTS API [å£°éŸ³={voice}, è¯­é€Ÿ={role_speed_factor}]: {text[:30]}...")
        
        try:
            # è°ƒç”¨TTS API
            response = SpeechSynthesizer.call(**params)
            
            # æ£€æŸ¥å“åº”çŠ¶æ€
            resp_dict = response.get_response()
            if resp_dict.get('status_code') == 200:
                # è·å–éŸ³é¢‘æ•°æ®
                audio_data = response.get_audio_data()
                
                if audio_data:
                    # ä¿å­˜éŸ³é¢‘æ–‡ä»¶
                    with open(output_path, 'wb') as f:
                        f.write(audio_data)
                    
                    print(f"âœ… è¯­éŸ³åˆæˆæˆåŠŸ: {output_path.name}")
                    return str(output_path)
                else:
                    raise Exception("éŸ³é¢‘æ•°æ®ä¸ºç©º")
            else:
                raise Exception(f"APIè°ƒç”¨å¤±è´¥: {resp_dict}")
                
        except Exception as e:
            print(f"âŒ Qwen TTSåˆæˆå¤±è´¥: {e}")
            raise
    
    def _select_qwen_model(self, text, role_config):
        """
        æ ¹æ®æ–‡æœ¬å†…å®¹å’Œè§’è‰²é…ç½®é€‰æ‹©åˆé€‚çš„Qwen TTSæ¨¡å‹
        
        Args:
            text: è¦åˆæˆçš„æ–‡æœ¬
            role_config: è§’è‰²é…ç½®
            
        Returns:
            str: æ¨¡å‹åç§°
        """
        # æ£€æŸ¥è§’è‰²é…ç½®ä¸­æ˜¯å¦æŒ‡å®šäº†æ¨¡å‹
        if 'model' in role_config:
            return role_config['model']
        
        # æ£€æŸ¥å…¨å±€text_langè®¾ç½®
        if hasattr(self, 'text_lang'):
            if self.text_lang == 'ja':
                return 'sambert-zhiying-v1'  # æ—¥è¯­æ¨¡å‹
            elif self.text_lang == 'en':
                return 'sambert-zhiying-v1'  # è‹±è¯­ä¹Ÿç”¨è¿™ä¸ªå¤šè¯­è¨€æ¨¡å‹
            elif self.text_lang == 'zh':
                return 'sambert-zhichu-v1'   # ä¸­æ–‡æ¨¡å‹
        
        # ç®€å•çš„è¯­è¨€æ£€æµ‹
        import re
        
        # æ£€æµ‹æ—¥è¯­å­—ç¬¦ï¼ˆå¹³å‡åã€ç‰‡å‡åã€æ±‰å­—ï¼‰
        japanese_pattern = r'[\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FAF]'
        if re.search(japanese_pattern, text):
            # è¿›ä¸€æ­¥æ£€æµ‹æ˜¯å¦åŒ…å«å‡åï¼ˆæ›´ç¡®å®šæ˜¯æ—¥è¯­ï¼‰
            kana_pattern = r'[\u3040-\u309F\u30A0-\u30FF]'
            if re.search(kana_pattern, text):
                print(f"   ğŸŒ æ£€æµ‹åˆ°æ—¥è¯­æ–‡æœ¬ï¼Œä½¿ç”¨æ—¥è¯­æ¨¡å‹")
                return 'sambert-zhiying-v1'
        
        # æ£€æµ‹è‹±è¯­å­—ç¬¦
        english_pattern = r'^[a-zA-Z\s\.,!?;:\'\"()-]+$'
        if re.match(english_pattern, text.strip()):
            print(f"   ğŸ‡ºğŸ‡¸ æ£€æµ‹åˆ°è‹±è¯­æ–‡æœ¬ï¼Œä½¿ç”¨å¤šè¯­è¨€æ¨¡å‹")
            return 'sambert-zhiying-v1'
        
        # é»˜è®¤ä½¿ç”¨ä¸­æ–‡æ¨¡å‹
        print(f"   ğŸ‡¨ğŸ‡³ é»˜è®¤ä½¿ç”¨ä¸­æ–‡æ¨¡å‹")
        return 'sambert-zhichu-v1'
    
    def create_silence(self, duration_ms):
        """åˆ›å»ºé™éŸ³éŸ³é¢‘"""
        return AudioSegment.silent(duration=duration_ms)
    
    def update_progress(self, current, total, subtitle_data=None):
        """æ›´æ–°ä»»åŠ¡è¿›åº¦"""
        if self.task_dict and self.task_id:
            progress = int((current / total) * 100)
            self.task_dict[self.task_id]["progress"] = progress
            if subtitle_data:
                self.task_dict[self.task_id]["current_subtitle"] = subtitle_data
    
    def process(self):
        """
        å¤„ç†å®Œæ•´çš„é…éŸ³æµç¨‹ï¼ˆæ”¯æŒåŒé‡å˜é€Ÿæœºåˆ¶ï¼‰
        
        Returns:
            dict: {
                'audio_path': str,  # æœ€ç»ˆéŸ³é¢‘æ–‡ä»¶è·¯å¾„
                'srt_path': str or None  # æ›´æ–°åçš„SRTæ–‡ä»¶è·¯å¾„ï¼ˆå¦‚æœæœ‰ï¼‰
            }
        """
        print("ğŸ¬ å¼€å§‹TTSé…éŸ³å¤„ç†...")
        
        # 1. è§£æSRTæ–‡ä»¶
        subtitles = self.parse_srt()
        total_subtitles = len(subtitles)
        
        if total_subtitles == 0:
            raise ValueError("SRTæ–‡ä»¶ä¸­æ²¡æœ‰å­—å¹•")
        
        # 2. åˆæˆæ¯æ¡å­—å¹•çš„è¯­éŸ³
        audio_files = []
        subtitle_data = []
        
        for i, subtitle in enumerate(subtitles):
            # æ›´æ–°è¿›åº¦
            self.update_progress(i, total_subtitles, subtitle)
            
            print(f"\nğŸ“ å¤„ç†å­—å¹• {i+1}/{total_subtitles}: {subtitle['text'][:50]}...")
            
            # è·å–æ—¶é—´ä¿¡æ¯
            start_ms = self.time_to_ms(subtitle['start'])
            end_ms = self.time_to_ms(subtitle['end'])
            target_duration_ms = end_ms - start_ms
            
            # åˆæˆè¯­éŸ³ï¼ˆä¼ é€’è¯´è¯äººä¿¡æ¯å’Œç›®æ ‡æ—¶é•¿ï¼‰
            speaker = subtitle.get('speaker', None)
            
            # å¦‚æœå¯ç”¨è‡ªåŠ¨å¯¹é½ï¼Œä¼ å…¥ç›®æ ‡æ—¶é•¿è®©TTSè‡ªåŠ¨è°ƒæ•´è¯­é€Ÿ
            if self.auto_align:
                audio_path = self.synthesize_speech(subtitle['text'], i + 1, speaker, target_duration_ms)
            else:
                audio_path = self.synthesize_speech(subtitle['text'], i + 1, speaker)
            
            audio_files.append(audio_path)
            
            # æµ‹é‡å®é™…éŸ³é¢‘æ—¶é•¿ï¼ˆæ–¹æ¡ˆBéœ€è¦ï¼‰
            from pydub import AudioSegment
            actual_audio = AudioSegment.from_file(audio_path)
            actual_duration_ms = len(actual_audio)
            
            # æ„å»ºå­—å¹•æ•°æ®ï¼ˆç”¨äºåŒé‡å˜é€Ÿï¼‰
            subtitle_data.append({
                'start_ms': start_ms,
                'end_ms': end_ms,
                'text': subtitle['text'],
                'audio_file': audio_path,
                'speaker': speaker,
                'original_duration_ms': target_duration_ms,  # åŸå§‹å­—å¹•æ—¶é•¿
                'actual_duration_ms': actual_duration_ms     # å®é™…éŸ³é¢‘æ—¶é•¿
            })
        
        # 3. åˆ¤æ–­æ˜¯å¦ä½¿ç”¨ä¿æŒæ€»æ—¶é•¿åŠŸèƒ½ï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼‰
        print(f"\nğŸ” è°ƒè¯•ä¿¡æ¯:")
        print(f"   preserve_total_time = {self.preserve_total_time}")
        print(f"   enable_smart_speedup = {self.enable_smart_speedup}")
        print(f"   auto_align = {self.auto_align}")
        
        if self.preserve_total_time:
            print("\nğŸš€ å¯ç”¨ä¿æŒSRTæ€»æ—¶é•¿ä¸å˜åŠŸèƒ½...")
            
            # ä½¿ç”¨TimelineAdjusteråŠ¨æ€è°ƒæ•´æ—¶é—´è½´
            if True:
                print("\n" + "â±ï¸ "*30)
                print("â±ï¸  ä½¿ç”¨åŠ¨æ€æ—¶é—´è½´è°ƒæ•´ï¼ˆä¿æŒæ€»æ—¶é•¿ï¼‰")
                print(f"ğŸ“Š åŸå§‹SRTæ€»æ—¶é•¿: {subtitle_data[-1]['end_ms']}ms")
                print(f"ğŸ“Š å­—å¹•æ•°é‡: {len(subtitle_data)}")
                print(f"ğŸ“Š é…éŸ³æ–‡ä»¶æ•°é‡: {len(audio_files)}")
                print(f"ğŸ“Š è¯­é€Ÿç³»æ•°: {self.speed_factor}")
                print("â±ï¸ "*30 + "\n")
                
                # ä½¿ç”¨TimelineAdjusteråŠ¨æ€è°ƒæ•´æ—¶é—´è½´ï¼ˆå¸¦è¯­é€Ÿé™åˆ¶ï¼‰
                timeline_adjuster = TimelineAdjuster(
                    subtitles=subtitle_data,
                    audio_files=audio_files,
                    preserve_total_time=True,
                    target_speed_factor=self.speed_factor,
                    max_speed_limit=2.0  # é™åˆ¶æœ€å¤§è¯­é€Ÿä¸º2.0x
                )
                
                # è°ƒæ•´æ—¶é—´è½´
                updated_subtitles = timeline_adjuster.adjust_timeline()
                
                # è¾“å‡ºè°ƒæ•´ç»“æœ
                if updated_subtitles:
                    final_time = updated_subtitles[-1]['end_ms']
                    original_time = subtitle_data[-1]['end_ms']
                    print(f"\nğŸ“Š è°ƒæ•´ç»“æœ:")
                    print(f"   åŸå§‹æ€»æ—¶é•¿: {original_time}ms")
                    print(f"   è°ƒæ•´åæ€»æ—¶é•¿: {final_time}ms")
                    print(f"   æ—¶é•¿å·®å¼‚: {final_time - original_time:+d}ms")
                    if abs(final_time - original_time) < 100:
                        print(f"   âœ… æ€»æ—¶é•¿ä¿æŒä¸€è‡´ï¼ˆè¯¯å·® < 0.1ç§’ï¼‰")
                    else:
                        print(f"   âš ï¸ æ€»æ—¶é•¿æœ‰å·®å¼‚ï¼ˆè¯¯å·® = {abs(final_time - original_time)}msï¼‰")
                
                # æ ¹æ®æ›´æ–°åçš„æ—¶é—´è½´åˆå¹¶éŸ³é¢‘
                output_path = self._merge_audio_with_timeline(updated_subtitles, audio_files)
                
                # ä¿å­˜æ›´æ–°åçš„å­—å¹•
                updated_srt_path = self._save_updated_srt(updated_subtitles)
        
        # 4. åˆ¤æ–­æ˜¯å¦ä½¿ç”¨æ™ºèƒ½åŒé‡å˜é€Ÿæœºåˆ¶ï¼ˆä¸ä¿æŒæ€»æ—¶é•¿ï¼‰
        elif self.enable_smart_speedup:
            print("\nğŸš€ å¯ç”¨æ™ºèƒ½åŒé‡å˜é€Ÿæœºåˆ¶...")
            
            if False:  # è¿™ä¸ªåˆ†æ”¯å·²ç»è¢«ä¸Šé¢çš„preserve_total_timeå¤„ç†äº†
                pass
            else:
                print("âš¡ ä½¿ç”¨ä¼ ç»ŸåŒé‡å˜é€Ÿæœºåˆ¶ï¼ˆä¸ä¿æŒæ€»æ—¶é•¿ï¼‰")
                
                # è®¡ç®—åŸå§‹è§†é¢‘æ€»æ—¶é•¿
                raw_total_time_ms = subtitle_data[-1]['end_ms'] if subtitle_data else 0
                
                # åˆ›å»ºåŒé‡å˜é€Ÿè°ƒæ•´å™¨
                adjuster = SpeedRateAdjuster(
                    subtitles=subtitle_data,
                    audio_files=audio_files,
                    output_dir=str(self.output_dir),
                    enable_audio_speedup=self.enable_audio_speedup,
                    enable_video_slowdown=self.enable_video_slowdown,
                    max_audio_speed_rate=self.max_audio_speed_rate,
                    max_video_pts_rate=self.max_video_pts_rate,
                    remove_silent_gaps=self.remove_silent_gaps,
                    align_subtitle_audio=self.auto_align,
                    raw_total_time_ms=raw_total_time_ms
                )
                
                # æ‰§è¡ŒåŒé‡å˜é€Ÿå¤„ç†
                output_path, updated_subtitles = adjuster.process()
                
                # ä¿å­˜æ›´æ–°åçš„å­—å¹•
                updated_srt_path = self._save_updated_srt(updated_subtitles)
        
        # 5. ä½¿ç”¨ä¼ ç»Ÿæ–¹å¼æ‹¼æ¥éŸ³é¢‘
        else:
            print("\nğŸ”— ä½¿ç”¨ä¼ ç»Ÿæ–¹å¼æ‹¼æ¥éŸ³é¢‘ï¼ˆå¼ºåˆ¶ä¿ç•™åŸå§‹é—´éš”ï¼‰...")
            print(f"   ç­–ç•¥ï¼šé¡ºåºæ‹¼æ¥ï¼Œå¼ºåˆ¶ä¿ç•™åŸå§‹SRTé—´éš”")
            
            audio_segments = []
            
            # æ­¥éª¤0ï¼šæ·»åŠ ç¬¬ä¸€æ¡å­—å¹•å‰çš„åˆå§‹ç©ºç™½
            if subtitle_data:
                first_start_ms = subtitle_data[0]['start_ms']
                if first_start_ms > 0:
                    print(f"   â±ï¸  æ·»åŠ ç¬¬ä¸€æ¡å­—å¹•å‰çš„åˆå§‹ç©ºç™½: {first_start_ms}ms ({first_start_ms/1000:.1f}ç§’)")
                    audio_segments.append(self.create_silence(first_start_ms))
            
            for i, subtitle_info in enumerate(subtitle_data):
                start_ms = subtitle_info['start_ms']  # åŸå§‹å¼€å§‹æ—¶é—´
                end_ms = subtitle_info['end_ms']      # åŸå§‹ç»“æŸæ—¶é—´
                
                # è®¡ç®—åŸå§‹é—´éš”ï¼ˆå¦‚æœä¸æ˜¯ç¬¬ä¸€æ¡ï¼‰
                if i > 0:
                    prev_subtitle = subtitle_data[i - 1]
                    original_gap = start_ms - prev_subtitle['end_ms']
                    
                    if original_gap > 0:
                        if i <= 5:
                            print(f"   å­—å¹•{i}åˆ°{i+1}æ·»åŠ åŸå§‹é—´éš”: {original_gap}ms ({original_gap/1000:.1f}ç§’)")
                        audio_segments.append(self.create_silence(original_gap))
                
                # åŠ è½½é…éŸ³éŸ³é¢‘
                audio = AudioSegment.from_wav(subtitle_info['audio_file'])
                audio_duration = len(audio)
                
                audio_segments.append(audio)
                
                # å…³é”®ä¿®å¤ï¼šå¦‚æœé…éŸ³æ—¶é•¿å°äºå­—å¹•æ—¶é•¿ï¼Œéœ€è¦å¡«å……é™éŸ³
                original_duration = end_ms - start_ms
                if audio_duration < original_duration:
                    padding_ms = original_duration - audio_duration
                    if i < 5:
                        print(f"   å­—å¹•{i+1}: é…éŸ³æ—¶é•¿={audio_duration}ms, åŸå§‹æ—¶é•¿={original_duration}ms, å¡«å……={padding_ms}ms")
                    audio_segments.append(self.create_silence(padding_ms))
                else:
                    if i < 5:
                        print(f"   å­—å¹•{i+1}: é…éŸ³æ—¶é•¿={audio_duration}ms ({audio_duration/1000:.1f}ç§’)")
                
                if i == 5:
                    print(f"   ... (çœç•¥åç»­å­—å¹•)")
            
            # æ­¥éª¤Nï¼šéªŒè¯éŸ³é¢‘æ€»æ—¶é•¿ï¼ˆä¸å†éœ€è¦æ‰‹åŠ¨æ·»åŠ å°¾éƒ¨ç©ºç™½ï¼Œå› ä¸ºå·²ç»åœ¨å¾ªç¯ä¸­å¤„ç†äº†ï¼‰
            if subtitle_data:
                last_subtitle = subtitle_data[-1]
                expected_duration_ms = last_subtitle['end_ms']  # æœŸæœ›çš„æ€»æ—¶é•¿
                
                # è®¡ç®—å½“å‰éŸ³é¢‘çš„å®é™…æ—¶é•¿
                actual_duration_ms = sum(len(seg) for seg in audio_segments)
                
                duration_diff = actual_duration_ms - expected_duration_ms
                
                if abs(duration_diff) < 100:  # è¯¯å·®å°äº100ms
                    print(f"\n   âœ… éŸ³é¢‘æ—¶é•¿éªŒè¯é€šè¿‡:")
                    print(f"      æœŸæœ›: {expected_duration_ms}ms ({expected_duration_ms/1000:.1f}ç§’)")
                    print(f"      å®é™…: {actual_duration_ms}ms ({actual_duration_ms/1000:.1f}ç§’)")
                    print(f"      è¯¯å·®: {duration_diff:+d}ms")
                else:
                    print(f"\n   âš ï¸  éŸ³é¢‘æ—¶é•¿æœ‰å·®å¼‚:")
                    print(f"      æœŸæœ›: {expected_duration_ms}ms ({expected_duration_ms/1000:.1f}ç§’)")
                    print(f"      å®é™…: {actual_duration_ms}ms ({actual_duration_ms/1000:.1f}ç§’)")
                    print(f"      å·®å¼‚: {duration_diff:+d}ms ({duration_diff/1000:+.1f}ç§’)")
                    
                    # å¦‚æœå®é™…æ—¶é•¿å°äºæœŸæœ›ï¼Œæ·»åŠ å°¾éƒ¨ç©ºç™½è¡¥é½
                    if actual_duration_ms < expected_duration_ms:
                        tail_padding = expected_duration_ms - actual_duration_ms
                        print(f"      ğŸ”§ æ·»åŠ å°¾éƒ¨å¡«å……: {tail_padding}ms")
                        audio_segments.append(self.create_silence(tail_padding))
            
            # æ‹¼æ¥æ‰€æœ‰éŸ³é¢‘
            if not audio_segments:
                raise ValueError("æ²¡æœ‰éŸ³é¢‘ç‰‡æ®µå¯ä»¥æ‹¼æ¥")
            
            final_audio = audio_segments[0]
            for segment in audio_segments[1:]:
                final_audio += segment
            
            # å¯¼å‡ºæœ€ç»ˆéŸ³é¢‘
            output_path = self.output_dir / "dubbing_result.wav"
            final_duration = len(final_audio)
            print(f"\nğŸ’¾ å¯¼å‡ºæœ€ç»ˆéŸ³é¢‘: {output_path}")
            print(f"   æ€»æ—¶é•¿: {final_duration}ms ({final_duration/1000:.1f}ç§’)")
            
            final_audio.export(output_path, format="wav")
            output_path = str(output_path)
            
            # ç”Ÿæˆå­—å¹•æ–‡ä»¶
            updated_srt_path = None
            if self.remove_silent_gaps:
                # æ–¹æ¡ˆBï¼šåŸºäºå®é™…éŸ³é¢‘ç‰‡æ®µæ—¶é•¿ç”Ÿæˆç²¾ç¡®å­—å¹•ï¼ˆç§»é™¤é—´éš™ï¼‰
                updated_srt_path = self._generate_precise_subtitle_from_segments(
                    subtitle_data,
                    min_gap_ms=300  # ç‰‡æ®µä¹‹é—´ä¿ç•™300msé—´éš™
                )
            else:
                # æ–¹æ¡ˆDï¼šä¼ ç»Ÿæ¨¡å¼ - æ ¹æ®å®é™…æ‹¼æ¥çš„éŸ³é¢‘ç”Ÿæˆå­—å¹•
                # è€ƒè™‘è¯­é€Ÿè°ƒæ•´å’Œé™éŸ³é—´éš”
                updated_srt_path = self._generate_traditional_subtitle(
                    subtitle_data,
                    silence_duration_ms=int(self.silence_duration * 1000)
                )
        
        # 6. æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        print("ğŸ§¹ æ¸…ç†ä¸´æ—¶æ–‡ä»¶...")
        for temp_file in self.temp_dir.glob("*.wav"):
            temp_file.unlink()
        
        print(f"âœ… TTSé…éŸ³å®Œæˆï¼è¾“å‡ºæ–‡ä»¶: {output_path}")
        if updated_srt_path:
            print(f"âœ… æ›´æ–°åçš„å­—å¹•: {updated_srt_path}")
        
        return {
            'audio_path': output_path,
            'srt_path': updated_srt_path
        }
    
    def _speedup_audio_ffmpeg(self, input_file, output_file, speed_ratio, target_duration_ms):
        """
        ä½¿ç”¨FFmpegé«˜è´¨é‡åŠ é€ŸéŸ³é¢‘
        
        Args:
            input_file: è¾“å…¥éŸ³é¢‘æ–‡ä»¶
            output_file: è¾“å‡ºéŸ³é¢‘æ–‡ä»¶
            speed_ratio: åŠ é€Ÿå€ç‡
            target_duration_ms: ç›®æ ‡æ—¶é•¿ï¼ˆæ¯«ç§’ï¼‰
        
        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        try:
            import subprocess
            
            target_duration_sec = target_duration_ms / 1000.0
            
            # å°è¯•ä½¿ç”¨ rubberband æ»¤é•œï¼ˆé«˜è´¨é‡ï¼‰
            cmd = [
                'ffmpeg', '-y', '-i', input_file,
                '-filter:a', f'rubberband=tempo={speed_ratio}',
                '-t', f'{target_duration_sec:.4f}',
                '-ar', '44100',
                '-ac', '2',
                '-c:a', 'pcm_s16le',
                output_file
            ]
            
            result = subprocess.run(cmd, capture_output=True, text=True, encoding='utf-8', errors='ignore')
            
            if result.returncode != 0:
                # rubberband ä¸å¯ç”¨ï¼Œä½¿ç”¨ atempo
                print(f"    âš ï¸ rubberband ä¸å¯ç”¨ï¼Œä½¿ç”¨ atempo")
                
                # atempo é™åˆ¶åœ¨ 0.5-2.0 ä¹‹é—´ï¼Œéœ€è¦é“¾å¼å¤„ç†
                tempo_filters = []
                current_tempo = speed_ratio
                while current_tempo > 2.0:
                    tempo_filters.append("atempo=2.0")
                    current_tempo /= 2.0
                while current_tempo < 0.5:
                    tempo_filters.append("atempo=0.5")
                    current_tempo /= 0.5
                if 0.5 <= current_tempo <= 2.0:
                    tempo_filters.append(f"atempo={current_tempo}")
                
                filter_str = ",".join(tempo_filters)
                
                cmd = [
                    'ffmpeg', '-y', '-i', input_file,
                    '-filter:a', filter_str,
                    '-t', f'{target_duration_sec:.4f}',
                    '-ar', '44100',
                    '-ac', '2',
                    '-c:a', 'pcm_s16le',
                    output_file
                ]
                
                result = subprocess.run(cmd, capture_output=True, text=True, encoding='utf-8', errors='ignore')
                
                if result.returncode != 0:
                    print(f"    âŒ FFmpeg åŠ é€Ÿå¤±è´¥: {result.stderr}")
                    return False
            
            return True
            
        except Exception as e:
            print(f"    âŒ éŸ³é¢‘åŠ é€Ÿå¼‚å¸¸: {e}")
            return False
    
    def _merge_audio_with_timeline(self, updated_subtitles, audio_files):
        """
        æ ¹æ®æ›´æ–°åçš„æ—¶é—´è½´åˆå¹¶éŸ³é¢‘ï¼Œå¹¶éªŒè¯æ—¶é•¿å‡†ç¡®æ€§
        
        Args:
            updated_subtitles: æ›´æ–°åçš„å­—å¹•åˆ—è¡¨ï¼ˆåŒ…å«æ–°çš„start_mså’Œend_msï¼‰
            audio_files: é…éŸ³æ–‡ä»¶åˆ—è¡¨
            
        Returns:
            æœ€ç»ˆéŸ³é¢‘æ–‡ä»¶è·¯å¾„
        """
        print("\nğŸ”— æ ¹æ®åŠ¨æ€æ—¶é—´è½´åˆå¹¶éŸ³é¢‘...")
        
        audio_segments = []
        current_time = 0
        
        # åˆ›å»ºä¸´æ—¶ç›®å½•ç”¨äºå­˜æ”¾åŠ é€Ÿåçš„éŸ³é¢‘
        speedup_temp_dir = self.temp_dir / "speedup"
        speedup_temp_dir.mkdir(parents=True, exist_ok=True)
        
        # ç”¨äºéªŒè¯çš„ç»Ÿè®¡ä¿¡æ¯
        total_audio_duration = 0
        total_gap_duration = 0
        duration_mismatches = []
        
        for i, subtitle in enumerate(updated_subtitles):
            # æ·»åŠ å­—å¹•å‰çš„é™éŸ³é—´éš™
            if subtitle['start_ms'] > current_time:
                gap = subtitle['start_ms'] - current_time
                print(f"  å­—å¹• {i+1} å‰æ·»åŠ é™éŸ³: {gap}ms")
                audio_segments.append(AudioSegment.silent(duration=gap))
                current_time += gap
                total_gap_duration += gap
            
            # åŠ è½½é…éŸ³éŸ³é¢‘
            audio_file = audio_files[i] if i < len(audio_files) else None
            if audio_file and os.path.exists(audio_file):
                try:
                    audio = AudioSegment.from_file(audio_file)
                    audio_duration = len(audio)
                    
                    # è®¡ç®—ç›®æ ‡æ—¶é•¿
                    target_duration = subtitle['end_ms'] - subtitle['start_ms']
                    
                    # æ£€æŸ¥æ˜¯å¦éœ€è¦åŠ é€Ÿï¼ˆä½¿ç”¨ original_duration_ms å’Œ adjusted_duration_msï¼‰
                    original_duration = subtitle.get('original_duration_ms', audio_duration)
                    adjusted_duration = subtitle.get('adjusted_duration_ms', target_duration)
                    
                    # å¦‚æœè°ƒæ•´åæ—¶é•¿ < åŸå§‹æ—¶é•¿ï¼Œè¯´æ˜éœ€è¦åŠ é€Ÿ
                    if original_duration > adjusted_duration and abs(original_duration - adjusted_duration) > 10:
                        speed_ratio = original_duration / adjusted_duration
                        print(f"  å­—å¹• {i+1}: åŠ é€ŸéŸ³é¢‘ {speed_ratio:.2f}x ({original_duration}ms -> {adjusted_duration}ms)")
                        
                        # ä½¿ç”¨FFmpegåŠ é€Ÿ
                        speedup_output = speedup_temp_dir / f"speedup_{i:04d}.wav"
                        if self._speedup_audio_ffmpeg(audio_file, str(speedup_output), speed_ratio, adjusted_duration):
                            # åŠ é€ŸæˆåŠŸï¼ŒåŠ è½½åŠ é€Ÿåçš„éŸ³é¢‘
                            audio = AudioSegment.from_file(str(speedup_output))
                            print(f"    âœ… åŠ é€ŸæˆåŠŸï¼Œå®é™…æ—¶é•¿: {len(audio)}ms")
                        else:
                            # åŠ é€Ÿå¤±è´¥ï¼Œä½¿ç”¨pydubçš„speedupä½œä¸ºå¤‡é€‰
                            print(f"    âš ï¸ FFmpegåŠ é€Ÿå¤±è´¥ï¼Œä½¿ç”¨pydubå¤‡é€‰æ–¹æ¡ˆ")
                            audio = audio.speedup(playback_speed=speed_ratio)
                    
                    # ç¡®ä¿éŸ³é¢‘æ—¶é•¿åŒ¹é…ç›®æ ‡æ—¶é•¿
                    actual_audio_duration = len(audio)
                    if abs(actual_audio_duration - target_duration) > 10:
                        if actual_audio_duration > target_duration:
                            # éŸ³é¢‘ä»ç„¶å¤ªé•¿ï¼Œæˆªæ–­
                            audio = audio[:target_duration]
                            print(f"    âš ï¸ éŸ³é¢‘ä»ç„¶å¤ªé•¿ï¼Œæˆªæ–­åˆ° {target_duration}ms")
                        else:
                            # éŸ³é¢‘å¤ªçŸ­ï¼Œæ·»åŠ å°¾éƒ¨é™éŸ³
                            padding = target_duration - actual_audio_duration
                            audio = audio + AudioSegment.silent(duration=padding)
                            print(f"    âš ï¸ éŸ³é¢‘å¤ªçŸ­ï¼Œæ·»åŠ å°¾éƒ¨é™éŸ³ {padding}ms")
                    
                    # è®°å½•æ—¶é•¿å·®å¼‚ï¼ˆç”¨äºéªŒè¯ï¼‰
                    final_audio_duration = len(audio)
                    if abs(final_audio_duration - target_duration) > 50:
                        duration_mismatches.append({
                            'index': i+1,
                            'expected': target_duration,
                            'actual': final_audio_duration,
                            'diff': final_audio_duration - target_duration
                        })
                    
                    audio_segments.append(audio)
                    current_time += len(audio)
                    total_audio_duration += len(audio)
                    print(f"  å­—å¹• {i+1}: æ·»åŠ é…éŸ³ {len(audio)}ms (é¢„æœŸ: {target_duration}ms)")
                    
                except Exception as e:
                    print(f"  âš ï¸ å­—å¹• {i+1} åŠ è½½éŸ³é¢‘å¤±è´¥: {e}ï¼Œä½¿ç”¨é™éŸ³")
                    silence_duration = subtitle['end_ms'] - subtitle['start_ms']
                    audio_segments.append(AudioSegment.silent(duration=silence_duration))
                    current_time += silence_duration
                    total_audio_duration += silence_duration
            else:
                # ä½¿ç”¨é™éŸ³å¡«å……
                silence_duration = subtitle['end_ms'] - subtitle['start_ms']
                print(f"  å­—å¹• {i+1}: ä½¿ç”¨é™éŸ³å¡«å…… {silence_duration}ms")
                audio_segments.append(AudioSegment.silent(duration=silence_duration))
                current_time += silence_duration
                total_audio_duration += silence_duration
        
        # åˆå¹¶æ‰€æœ‰éŸ³é¢‘ç‰‡æ®µ
        print(f"\n  ğŸ”— åˆå¹¶ {len(audio_segments)} ä¸ªéŸ³é¢‘ç‰‡æ®µ...")
        
        if not audio_segments:
            raise ValueError("æ²¡æœ‰éŸ³é¢‘ç‰‡æ®µå¯ä»¥æ‹¼æ¥")
        
        # ä½¿ç”¨ç¬¬ä¸€ä¸ªç‰‡æ®µä½œä¸ºèµ·ç‚¹ï¼Œç„¶åé€ä¸ªæ‹¼æ¥
        final_audio = audio_segments[0]
        for segment in audio_segments[1:]:
            final_audio += segment
        
        # å¯¼å‡ºæœ€ç»ˆéŸ³é¢‘
        output_path = self.output_dir / "dubbing_result.wav"
        print(f"  ğŸ’¾ å¯¼å‡ºæœ€ç»ˆéŸ³é¢‘: {output_path}")
        final_audio.export(str(output_path), format="wav")
        
        # éªŒè¯æœ€ç»ˆæ—¶é•¿
        final_duration = len(final_audio)
        expected_duration = updated_subtitles[-1]['end_ms'] if updated_subtitles else 0
        
        print(f"\nğŸ“Š éŸ³é¢‘æ‹¼æ¥éªŒè¯:")
        print(f"   é…éŸ³æ€»æ—¶é•¿: {total_audio_duration/1000:.2f}ç§’ ({total_audio_duration}ms)")
        print(f"   é—´éš™æ€»æ—¶é•¿: {total_gap_duration/1000:.2f}ç§’ ({total_gap_duration}ms)")
        print(f"   é¢„æœŸæ€»æ—¶é•¿: {expected_duration/1000:.2f}ç§’ ({expected_duration}ms)")
        print(f"   å®é™…æ€»æ—¶é•¿: {final_duration/1000:.2f}ç§’ ({final_duration}ms)")
        print(f"   å·®å¼‚: {abs(final_duration - expected_duration)/1000:.2f}ç§’ ({abs(final_duration - expected_duration)}ms)")
        
        if abs(final_duration - expected_duration) < 100:
            print(f"   âœ… æ—¶é•¿åŒ¹é…è‰¯å¥½ï¼ˆè¯¯å·® < 0.1ç§’ï¼‰")
        elif abs(final_duration - expected_duration) < 1000:
            print(f"   âš ï¸ æ—¶é•¿æœ‰å°å¹…å·®å¼‚ï¼ˆè¯¯å·® < 1ç§’ï¼‰")
        else:
            print(f"   âŒ æ—¶é•¿å·®å¼‚è¾ƒå¤§ï¼ˆè¯¯å·® >= 1ç§’ï¼‰")
        
        if duration_mismatches:
            print(f"\n   å‘ç° {len(duration_mismatches)} ä¸ªéŸ³é¢‘æ—¶é•¿ä¸åŒ¹é…:")
            for mismatch in duration_mismatches[:5]:
                print(f"      å­—å¹•{mismatch['index']}: é¢„æœŸ{mismatch['expected']}ms, å®é™…{mismatch['actual']}ms, å·®å¼‚{mismatch['diff']:+d}ms")
            if len(duration_mismatches) > 5:
                print(f"      ... è¿˜æœ‰ {len(duration_mismatches)-5} ä¸ªä¸åŒ¹é…")
        
        return str(output_path)
    
    def _generate_precise_subtitle_from_segments(self, subtitle_data, min_gap_ms=300):
        """
        åŸºäºéŸ³é¢‘ç‰‡æ®µçš„å®é™…æ—¶é•¿ç”Ÿæˆç²¾ç¡®çš„å­—å¹•æ–‡ä»¶ï¼ˆæ–¹æ¡ˆBï¼‰
        
        Args:
            subtitle_data: å­—å¹•æ•°æ®åˆ—è¡¨ï¼Œæ¯é¡¹åŒ…å« text, actual_duration_ms, speaker ç­‰
            min_gap_ms: ç‰‡æ®µä¹‹é—´çš„æœ€å°é—´éš™ï¼ˆæ¯«ç§’ï¼‰ï¼Œé»˜è®¤300ms
            
        Returns:
            str: ç”Ÿæˆçš„ç²¾ç¡®å­—å¹•æ–‡ä»¶è·¯å¾„
        """
        print(f"\nğŸ¯ ç”Ÿæˆç²¾ç¡®å­—å¹•ï¼ˆæ–¹æ¡ˆB - åŸºäºå®é™…éŸ³é¢‘æ—¶é•¿ï¼‰:")
        print(f"   å­—å¹•æ•°é‡: {len(subtitle_data)}")
        print(f"   æœ€å°é—´éš™: {min_gap_ms}ms")
        
        # ç´¯ç§¯è®¡ç®—æ¯æ¡å­—å¹•çš„æ–°æ—¶é—´è½´
        precise_subtitles = []
        current_time_ms = 0
        
        for i, segment in enumerate(subtitle_data):
            # è·å–å®é™…éŸ³é¢‘æ—¶é•¿
            actual_duration = segment.get('actual_duration_ms', 0)
            
            # è®¡ç®—æ–°çš„å¼€å§‹å’Œç»“æŸæ—¶é—´
            new_start_ms = current_time_ms
            new_end_ms = current_time_ms + actual_duration
            
            precise_subtitles.append({
                'index': i + 1,
                'start_ms': new_start_ms,
                'end_ms': new_end_ms,
                'text': segment['text'],
                'speaker': segment.get('speaker', None),
                'original_start_ms': segment['start_ms'],
                'original_end_ms': segment['end_ms'],
                'actual_duration_ms': actual_duration
            })
            
            # æ›´æ–°å½“å‰æ—¶é—´ï¼ˆåŠ ä¸ŠéŸ³é¢‘æ—¶é•¿å’Œæœ€å°é—´éš™ï¼‰
            current_time_ms = new_end_ms + min_gap_ms
            
            # æ‰“å°è°ƒæ•´ä¿¡æ¯ï¼ˆå‰5æ¡ï¼‰
            if i < 5:
                original_duration = segment['end_ms'] - segment['start_ms']
                print(f"   å­—å¹•{i+1}: {segment['start_ms']}ms â†’ {new_start_ms}ms, "
                      f"æ—¶é•¿ {original_duration}ms â†’ {actual_duration}ms")
        
        # ä¿å­˜ç²¾ç¡®å­—å¹•
        output_srt = self.output_dir / "precise_subtitles.srt"
        
        with open(output_srt, 'w', encoding='utf-8') as f:
            for subtitle in precise_subtitles:
                f.write(f"{subtitle['index']}\n")
                
                # è½¬æ¢æ¯«ç§’ä¸ºSRTæ—¶é—´æ ¼å¼
                start_time = self._ms_to_srt_time(subtitle['start_ms'])
                end_time = self._ms_to_srt_time(subtitle['end_ms'])
                
                f.write(f"{start_time} --> {end_time}\n")
                
                # å¦‚æœæœ‰è¯´è¯äººæ ‡è®°ï¼Œä¿ç•™å®ƒ
                if subtitle['speaker']:
                    f.write(f"[{subtitle['speaker']}] {subtitle['text']}\n\n")
                else:
                    f.write(f"{subtitle['text']}\n\n")
        
        # ç»Ÿè®¡ä¿¡æ¯
        total_audio_duration = sum(s['actual_duration_ms'] for s in precise_subtitles)
        total_gaps = (len(precise_subtitles) - 1) * min_gap_ms
        final_duration = precise_subtitles[-1]['end_ms'] + min_gap_ms if precise_subtitles else 0
        
        print(f"\nâœ… ç²¾ç¡®å­—å¹•ç”Ÿæˆå®Œæˆ:")
        print(f"   æ€»éŸ³é¢‘æ—¶é•¿: {total_audio_duration/1000:.2f}ç§’")
        print(f"   æ€»é—´éš™æ—¶é•¿: {total_gaps/1000:.2f}ç§’")
        print(f"   æœ€ç»ˆæ€»æ—¶é•¿: {final_duration/1000:.2f}ç§’")
        print(f"   ä¿å­˜ä½ç½®: {output_srt}")
        
        return str(output_srt)
    
    def _adjust_subtitle_timeline_for_audio(self, original_srt_path, audio_duration_ms):
        """
        æ ¹æ®å®é™…éŸ³é¢‘æ—¶é•¿è‡ªåŠ¨è°ƒæ•´å­—å¹•æ—¶é—´è½´ï¼ˆæ–¹æ¡ˆAï¼‰
        å½“TTSéŸ³é¢‘æ¯”åŸå­—å¹•æ—¶é—´è½´é•¿æ—¶ï¼ŒæŒ‰æ¯”ä¾‹æ‹‰ä¼¸å­—å¹•æ—¶é—´è½´
        
        Args:
            original_srt_path: åŸå§‹SRTæ–‡ä»¶è·¯å¾„
            audio_duration_ms: å®é™…éŸ³é¢‘æ€»æ—¶é•¿ï¼ˆæ¯«ç§’ï¼‰
            
        Returns:
            str: è°ƒæ•´åçš„SRTæ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœä¸éœ€è¦è°ƒæ•´åˆ™è¿”å›None
        """
        # è§£æåŸå§‹SRTè·å–åŸå§‹æ—¶é—´è½´
        print(f"\nğŸ” è§£æå­—å¹•æ–‡ä»¶: {original_srt_path}")
        
        # ä¸´æ—¶ä¿å­˜å½“å‰çš„srt_pathï¼Œç„¶åä½¿ç”¨ä¼ å…¥çš„è·¯å¾„
        original_srt_path_backup = self.srt_path
        self.srt_path = original_srt_path
        
        subtitles = self.parse_srt()
        
        # æ¢å¤åŸæ¥çš„srt_path
        self.srt_path = original_srt_path_backup
        
        if not subtitles:
            print(f"âš ï¸ æœªèƒ½è§£æå­—å¹•æ–‡ä»¶")
            return None
        
        print(f"âœ… æˆåŠŸè§£æ {len(subtitles)} æ¡å­—å¹•")
        
        # è·å–åŸå§‹å­—å¹•çš„æ€»æ—¶é•¿
        original_duration_ms = self.time_to_ms(subtitles[-1]['end'])
        
        # åˆ¤æ–­æ˜¯å¦éœ€è¦è°ƒæ•´ï¼ˆéŸ³é¢‘æ¯”å­—å¹•é•¿10%ä»¥ä¸Šï¼‰
        if audio_duration_ms <= original_duration_ms * 1.1:
            print(f"\nğŸ“Š å­—å¹•æ—¶é—´è½´æ— éœ€è°ƒæ•´:")
            print(f"   åŸå§‹å­—å¹•æ—¶é•¿: {original_duration_ms/1000:.2f}ç§’")
            print(f"   å®é™…éŸ³é¢‘æ—¶é•¿: {audio_duration_ms/1000:.2f}ç§’")
            print(f"   å·®å¼‚: {(audio_duration_ms - original_duration_ms)/1000:.2f}ç§’ (< 10%)")
            return None
        
        # è®¡ç®—æ‹‰ä¼¸æ¯”ä¾‹
        stretch_ratio = audio_duration_ms / original_duration_ms
        
        print(f"\nğŸ¯ è‡ªåŠ¨è°ƒæ•´å­—å¹•æ—¶é—´è½´ï¼ˆæ–¹æ¡ˆAï¼‰:")
        print(f"   åŸå§‹å­—å¹•æ—¶é•¿: {original_duration_ms/1000:.2f}ç§’ ({original_duration_ms}ms)")
        print(f"   å®é™…éŸ³é¢‘æ—¶é•¿: {audio_duration_ms/1000:.2f}ç§’ ({audio_duration_ms}ms)")
        print(f"   æ‹‰ä¼¸æ¯”ä¾‹: {stretch_ratio:.2f}x")
        
        # è°ƒæ•´æ¯æ¡å­—å¹•çš„æ—¶é—´æˆ³
        adjusted_subtitles = []
        for subtitle in subtitles:
            start_ms = self.time_to_ms(subtitle['start'])
            end_ms = self.time_to_ms(subtitle['end'])
            
            # æŒ‰æ¯”ä¾‹æ‹‰ä¼¸
            new_start_ms = int(start_ms * stretch_ratio)
            new_end_ms = int(end_ms * stretch_ratio)
            
            adjusted_subtitles.append({
                'index': subtitle['index'],
                'start_ms': new_start_ms,
                'end_ms': new_end_ms,
                'text': subtitle['text'],
                'speaker': subtitle.get('speaker', None)
            })
        
        # ä¿å­˜è°ƒæ•´åçš„å­—å¹•
        output_srt = self.output_dir / "adjusted_subtitles.srt"
        
        with open(output_srt, 'w', encoding='utf-8') as f:
            for subtitle in adjusted_subtitles:
                f.write(f"{subtitle['index']}\n")
                
                # è½¬æ¢æ¯«ç§’ä¸ºSRTæ—¶é—´æ ¼å¼
                start_time = self._ms_to_srt_time(subtitle['start_ms'])
                end_time = self._ms_to_srt_time(subtitle['end_ms'])
                
                f.write(f"{start_time} --> {end_time}\n")
                
                # å¦‚æœæœ‰è¯´è¯äººæ ‡è®°ï¼Œä¿ç•™å®ƒ
                if subtitle['speaker']:
                    f.write(f"[{subtitle['speaker']}] {subtitle['text']}\n\n")
                else:
                    f.write(f"{subtitle['text']}\n\n")
        
        # éªŒè¯è°ƒæ•´ç»“æœ
        final_duration_ms = adjusted_subtitles[-1]['end_ms']
        print(f"\nâœ… å­—å¹•æ—¶é—´è½´è°ƒæ•´å®Œæˆ:")
        print(f"   è°ƒæ•´åæ€»æ—¶é•¿: {final_duration_ms/1000:.2f}ç§’ ({final_duration_ms}ms)")
        print(f"   ä¸éŸ³é¢‘å·®å¼‚: {abs(final_duration_ms - audio_duration_ms)}ms")
        print(f"   ä¿å­˜ä½ç½®: {output_srt}")
        
        return str(output_srt)
    
    def _generate_traditional_subtitle(self, subtitle_data, silence_duration_ms=500):
        """
        ä¼ ç»Ÿæ¨¡å¼ï¼šæ ¹æ®å®é™…éŸ³é¢‘æ‹¼æ¥é€»è¾‘ç”Ÿæˆå­—å¹•æ–‡ä»¶ï¼ˆæ–¹æ¡ˆDï¼‰
        
        è¿™ä¸ªæ–¹æ³•å®Œå…¨æ¨¡æ‹ŸéŸ³é¢‘æ‹¼æ¥çš„é€»è¾‘ï¼Œç¡®ä¿å­—å¹•å’ŒéŸ³é¢‘å®Œå…¨åŒæ­¥ï¼š
        1. ç´¯ç§¯è®¡ç®—æ—¶é—´è½´
        2. æ·»åŠ åŸå§‹é—´éš™ï¼ˆä»SRTè¯»å–ï¼‰
        3. æ·»åŠ éŸ³é¢‘ç‰‡æ®µï¼ˆä½¿ç”¨å®é™…æ—¶é•¿ï¼‰
        
        Args:
            subtitle_data: å­—å¹•æ•°æ®åˆ—è¡¨ï¼Œæ¯é¡¹åŒ…å« text, actual_duration_ms, start_ms, end_ms ç­‰
            silence_duration_ms: å­—å¹•é—´çš„é™éŸ³é—´éš”ï¼ˆæ¯«ç§’ï¼‰ï¼Œé»˜è®¤500msï¼ˆæœªä½¿ç”¨ï¼Œä¿ç•™åŸå§‹é—´éš”ï¼‰
            
        Returns:
            str: ç”Ÿæˆçš„å­—å¹•æ–‡ä»¶è·¯å¾„
        """
        print(f"\nğŸ¯ ç”Ÿæˆä¼ ç»Ÿæ¨¡å¼å­—å¹•ï¼ˆæ–¹æ¡ˆD - ä¿®å¤ç‰ˆï¼‰:")
        print(f"   å­—å¹•æ•°é‡: {len(subtitle_data)}")
        print(f"   ç­–ç•¥: ç´¯ç§¯è®¡ç®—æ—¶é—´è½´ï¼Œä¿æŒåŸå§‹SRTé—´éš”")
        print(f"   è‡ªåŠ¨å¯¹é½: {self.auto_align}")
        
        # ç´¯ç§¯è®¡ç®—æ¯æ¡å­—å¹•çš„æ–°æ—¶é—´è½´ï¼ˆå®Œå…¨æ¨¡æ‹ŸéŸ³é¢‘æ‹¼æ¥é€»è¾‘ï¼‰
        traditional_subtitles = []
        
        # æ­¥éª¤0ï¼šå¤„ç†ç¬¬ä¸€æ¡å­—å¹•å‰çš„åˆå§‹ç©ºç™½æ—¶é—´
        if subtitle_data:
            first_start_ms = subtitle_data[0]['start_ms']
            if first_start_ms > 0:
                current_time_ms = first_start_ms
                print(f"   â±ï¸  ç¬¬ä¸€æ¡å­—å¹•å‰çš„åˆå§‹ç©ºç™½: {first_start_ms}ms ({first_start_ms/1000:.1f}ç§’)")
            else:
                current_time_ms = 0
        else:
            current_time_ms = 0
        
        for i, segment in enumerate(subtitle_data):
            # è·å–åŸå§‹æ—¶é—´ä¿¡æ¯
            original_start_ms = segment['start_ms']
            original_end_ms = segment['end_ms']
            original_duration_ms = original_end_ms - original_start_ms
            
            # æ­¥éª¤1ï¼šè®¡ç®—å¹¶æ·»åŠ åŸå§‹é—´éš”ï¼ˆå¦‚æœä¸æ˜¯ç¬¬ä¸€æ¡ï¼‰
            if i > 0:
                prev_segment = subtitle_data[i - 1]
                original_gap = original_start_ms - prev_segment['end_ms']
                
                if original_gap > 0:
                    # æ·»åŠ åŸå§‹é—´éš”åˆ°ç´¯ç§¯æ—¶é—´
                    current_time_ms += original_gap
                    if i <= 5:
                        print(f"   å­—å¹•{i}åˆ°{i+1}æ·»åŠ åŸå§‹é—´éš”: {original_gap}ms ({original_gap/1000:.1f}ç§’)")
            
            # æ­¥éª¤2ï¼šè·å–å®é™…éŸ³é¢‘æ—¶é•¿
            actual_duration_ms = segment.get('actual_duration_ms', original_duration_ms)
            
            # æ­¥éª¤3ï¼šè®¡ç®—æ–°çš„æ—¶é—´è½´ï¼ˆä½¿ç”¨ç´¯ç§¯æ—¶é—´ï¼‰
            new_start_ms = current_time_ms
            new_end_ms = current_time_ms + actual_duration_ms
            
            traditional_subtitles.append({
                'index': i + 1,
                'start_ms': new_start_ms,
                'end_ms': new_end_ms,
                'text': segment['text'],
                'speaker': segment.get('speaker', None),
                'original_start_ms': original_start_ms,
                'original_end_ms': original_end_ms,
                'original_duration_ms': original_duration_ms,
                'actual_duration_ms': actual_duration_ms
            })
            
            # æ­¥éª¤4ï¼šæ›´æ–°ç´¯ç§¯æ—¶é—´
            current_time_ms = new_end_ms
            
            # æ‰“å°è°ƒæ•´ä¿¡æ¯ï¼ˆå‰5æ¡ï¼‰
            if i < 5:
                print(f"   å­—å¹•{i+1}: å¼€å§‹={new_start_ms}ms ({new_start_ms/1000:.2f}s), "
                      f"ç»“æŸ={new_end_ms}ms ({new_end_ms/1000:.2f}s), "
                      f"æ—¶é•¿={actual_duration_ms}ms ({actual_duration_ms/1000:.2f}s)")
                print(f"           åŸå§‹: {original_start_ms}ms-{original_end_ms}ms "
                      f"(æ—¶é•¿{original_duration_ms}ms)")
            elif i == 5:
                print(f"   ... (çœç•¥åç»­å­—å¹•)")
        
        # ä¿å­˜å­—å¹•
        output_srt = self.output_dir / "traditional_subtitles.srt"
        
        with open(output_srt, 'w', encoding='utf-8') as f:
            for subtitle in traditional_subtitles:
                f.write(f"{subtitle['index']}\n")
                
                # è½¬æ¢æ¯«ç§’ä¸ºSRTæ—¶é—´æ ¼å¼
                start_time = self._ms_to_srt_time(subtitle['start_ms'])
                end_time = self._ms_to_srt_time(subtitle['end_ms'])
                
                f.write(f"{start_time} --> {end_time}\n")
                
                # å¦‚æœæœ‰è¯´è¯äººæ ‡è®°ï¼Œä¿ç•™å®ƒ
                if subtitle['speaker']:
                    f.write(f"[{subtitle['speaker']}] {subtitle['text']}\n\n")
                else:
                    f.write(f"{subtitle['text']}\n\n")
        
        # ç»Ÿè®¡ä¿¡æ¯
        total_subtitle_duration = sum(s['actual_duration_ms'] for s in traditional_subtitles)
        final_duration = current_time_ms  # ä½¿ç”¨ç´¯ç§¯æ—¶é—´ä½œä¸ºæœ€ç»ˆæ—¶é•¿
        
        # è®¡ç®—æ€»é—´éš”
        total_gaps = 0
        for i in range(1, len(subtitle_data)):
            original_gap = subtitle_data[i]['start_ms'] - subtitle_data[i-1]['end_ms']
            if original_gap > 0:
                total_gaps += original_gap
        
        original_total_duration = subtitle_data[-1]['end_ms'] if subtitle_data else 0
        
        print(f"\nâœ… ä¼ ç»Ÿæ¨¡å¼å­—å¹•ç”Ÿæˆå®Œæˆ:")
        print(f"   åŸå§‹SRTæ€»æ—¶é•¿: {original_total_duration/1000:.2f}ç§’ ({original_total_duration}ms)")
        print(f"   é…éŸ³æ€»æ—¶é•¿: {total_subtitle_duration/1000:.2f}ç§’ ({total_subtitle_duration}ms)")
        print(f"   é—´éš”æ€»æ—¶é•¿: {total_gaps/1000:.2f}ç§’ ({total_gaps}ms)")
        print(f"   æœ€ç»ˆæ€»æ—¶é•¿: {final_duration/1000:.2f}ç§’ ({final_duration}ms)")
        print(f"   æ—¶é•¿å˜åŒ–: {(final_duration - original_total_duration)/1000:+.2f}ç§’")
        
        # éªŒè¯é—´éš”æ˜¯å¦ä¿æŒ
        if len(traditional_subtitles) > 1:
            print(f"\nğŸ” é—´éš”éªŒè¯ï¼ˆå‰3ä¸ªï¼‰:")
            for i in range(1, min(4, len(traditional_subtitles))):
                new_gap = traditional_subtitles[i]['start_ms'] - traditional_subtitles[i-1]['end_ms']
                original_gap = subtitle_data[i]['start_ms'] - subtitle_data[i-1]['end_ms']
                match = "âœ…" if abs(new_gap - original_gap) < 1 else "âŒ"
                print(f"   å­—å¹•{i}åˆ°{i+1}: åŸå§‹é—´éš”={original_gap}ms, æ–°é—´éš”={new_gap}ms {match}")
        
        print(f"   ä¿å­˜ä½ç½®: {output_srt}")
        
        return str(output_srt)
    
    def _save_updated_srt(self, subtitles):
        """
        ä¿å­˜æ›´æ–°åçš„å­—å¹•æ–‡ä»¶ï¼Œå¹¶éªŒè¯æ—¶é—´è½´
        
        Returns:
            str: ä¿å­˜çš„SRTæ–‡ä»¶è·¯å¾„
        """
        output_srt = self.output_dir / "updated_subtitles.srt"
        
        with open(output_srt, 'w', encoding='utf-8') as f:
            for i, subtitle in enumerate(subtitles):
                f.write(f"{i+1}\n")
                
                # è½¬æ¢æ¯«ç§’ä¸ºSRTæ—¶é—´æ ¼å¼
                start_time = self._ms_to_srt_time(subtitle['start_ms'])
                end_time = self._ms_to_srt_time(subtitle['end_ms'])
                
                f.write(f"{start_time} --> {end_time}\n")
                f.write(f"{subtitle['text']}\n\n")
        
        # éªŒè¯æ—¶é—´è½´
        print(f"\nğŸ“Š å­—å¹•æ—¶é—´è½´éªŒè¯:")
        total_subtitle_duration = sum(s['end_ms'] - s['start_ms'] for s in subtitles)
        total_timeline = subtitles[-1]['end_ms'] if subtitles else 0
        
        print(f"   å­—å¹•æ€»æ•°: {len(subtitles)}")
        print(f"   å­—å¹•æ€»æ—¶é•¿: {total_subtitle_duration/1000:.2f}ç§’ ({total_subtitle_duration}ms)")
        print(f"   æ—¶é—´è½´æ€»é•¿: {total_timeline/1000:.2f}ç§’ ({total_timeline}ms)")
        print(f"   é—´éš™æ€»æ—¶é•¿: {(total_timeline - total_subtitle_duration)/1000:.2f}ç§’ ({total_timeline - total_subtitle_duration}ms)")
        
        # æ£€æŸ¥å¼‚å¸¸
        warnings = []
        for i, sub in enumerate(subtitles):
            duration = sub['end_ms'] - sub['start_ms']
            if duration < 100:
                warnings.append(f"   âš ï¸ å­—å¹•{i+1}æ—¶é•¿è¿‡çŸ­: {duration}ms")
            if i > 0:
                gap = sub['start_ms'] - subtitles[i-1]['end_ms']
                if gap < 0:
                    warnings.append(f"   âš ï¸ å­—å¹•{i}å’Œ{i+1}é‡å : {abs(gap)}ms")
                elif gap > 5000:
                    warnings.append(f"   âš ï¸ å­—å¹•{i}å’Œ{i+1}é—´éš™è¿‡å¤§: {gap}ms ({gap/1000:.1f}ç§’)")
        
        if warnings:
            print(f"\n   å‘ç° {len(warnings)} ä¸ªè­¦å‘Š:")
            for warning in warnings[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                print(warning)
            if len(warnings) > 5:
                print(f"   ... è¿˜æœ‰ {len(warnings)-5} ä¸ªè­¦å‘Š")
        else:
            print(f"   âœ… æœªå‘ç°å¼‚å¸¸")
        
        print(f"\nğŸ’¾ ä¿å­˜æ›´æ–°åçš„å­—å¹•: {output_srt}")
        return str(output_srt)
    
    def _ms_to_srt_time(self, ms):
        """å°†æ¯«ç§’è½¬æ¢ä¸ºSRTæ—¶é—´æ ¼å¼"""
        hours = int(ms // 3600000)
        minutes = int((ms % 3600000) // 60000)
        seconds = int((ms % 60000) // 1000)
        milliseconds = int(ms % 1000)
        return f"{hours:02d}:{minutes:02d}:{seconds:02d},{milliseconds:03d}"


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    processor = TTSDubbingProcessor(
        srt_path="test.srt",
        output_dir="output",
        engine="gpt-sovits",
        role_data={
            "refAudioPath": "cs3.mp3",
            "promptText": "æµ‹è¯•æ–‡æœ¬",
            "promptLang": "zh"
        },
        api_url="http://192.168.110.204:9880"
    )
    
    result = processor.process()
    print(f"ç»“æœ: {result}")
