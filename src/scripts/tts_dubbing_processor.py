"""
TTSé…éŸ³å¤„ç†å™¨
è§£æSRTå­—å¹•æ–‡ä»¶ï¼Œè°ƒç”¨TTS APIç”Ÿæˆè¯­éŸ³ï¼ŒæŒ‰æ—¶é—´è½´æ‹¼æ¥éŸ³é¢‘
æ”¯æŒåŒé‡å˜é€Ÿæœºåˆ¶ï¼šæ™ºèƒ½éŸ³é¢‘åŠ é€Ÿå’Œè§†é¢‘æ…¢é€Ÿ
"""

import os
import re
import requests
from pathlib import Path
from pydub import AudioSegment
from pydub.generators import Sine
import time
from speed_rate_adjuster import SpeedRateAdjuster
from timeline_adjuster import TimelineAdjuster


class TTSDubbingProcessor:
    def __init__(self, srt_path, output_dir, engine, role_data, text_lang='zh',
                 speed_factor=1.0, silence_duration=0.5, auto_align=True, 
                 api_url=None, api_key=None, task_id=None, task_dict=None,
                 enable_smart_speedup=False, enable_audio_speedup=True, 
                 enable_video_slowdown=False, max_audio_speed_rate=2.0,
                 max_video_pts_rate=10.0, remove_silent_gaps=False,
                 preserve_total_time=True):
        """
        åˆå§‹åŒ–TTSé…éŸ³å¤„ç†å™¨
        
        Args:
            srt_path: SRTå­—å¹•æ–‡ä»¶è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•
            engine: TTSå¼•æ“ ('gpt-sovits' æˆ– 'qwen-tts')
            role_data: è§’è‰²é…ç½®æ•°æ®
            text_lang: åˆæˆè¯­è¨€ ('zh', 'en', 'ja', 'ko')
            speed_factor: è¯­é€Ÿç³»æ•°
            silence_duration: é™éŸ³é—´éš”æ—¶é•¿(ç§’)
            auto_align: æ˜¯å¦è‡ªåŠ¨å¯¹é½æ—¶é—´è½´
            api_url: GPT-SoVITS APIåœ°å€
            api_key: QwenTTS APIå¯†é’¥
            task_id: ä»»åŠ¡ID
            task_dict: ä»»åŠ¡çŠ¶æ€å­—å…¸
            enable_smart_speedup: æ˜¯å¦å¯ç”¨æ™ºèƒ½åŒé‡å˜é€Ÿæœºåˆ¶
            enable_audio_speedup: æ˜¯å¦å¯ç”¨éŸ³é¢‘åŠ é€Ÿ
            enable_video_slowdown: æ˜¯å¦å¯ç”¨è§†é¢‘æ…¢é€Ÿ
            max_audio_speed_rate: éŸ³é¢‘æœ€å¤§åŠ é€Ÿå€ç‡
            max_video_pts_rate: è§†é¢‘æœ€å¤§æ…¢é€Ÿå€ç‡
            remove_silent_gaps: æ˜¯å¦ç§»é™¤å­—å¹•é—´é™éŸ³é—´éš™
            preserve_total_time: æ˜¯å¦ä¿æŒSRTæ€»æ—¶é•¿ä¸å˜ï¼ˆåŠ¨æ€è°ƒæ•´æ—¶é—´è½´ï¼‰
        """
        self.srt_path = srt_path
        self.output_dir = Path(output_dir)
        self.engine = engine
        self.role_data = role_data
        self.text_lang = text_lang  # æ–°å¢ï¼šåˆæˆè¯­è¨€
        self.speed_factor = speed_factor
        self.silence_duration = silence_duration
        self.auto_align = auto_align
        self.api_url = api_url
        self.api_key = api_key
        self.task_id = task_id
        self.task_dict = task_dict
        
        # åŒé‡å˜é€Ÿæœºåˆ¶å‚æ•°
        self.enable_smart_speedup = enable_smart_speedup
        self.enable_audio_speedup = enable_audio_speedup
        self.enable_video_slowdown = enable_video_slowdown
        self.max_audio_speed_rate = max_audio_speed_rate
        self.max_video_pts_rate = max_video_pts_rate
        self.remove_silent_gaps = remove_silent_gaps
        self.preserve_total_time = preserve_total_time
        
        # åˆ›å»ºä¸´æ—¶ç›®å½•
        self.temp_dir = self.output_dir / "temp_audio"
        self.temp_dir.mkdir(parents=True, exist_ok=True)
        
    def parse_srt(self):
        """è§£æSRTæ–‡ä»¶ï¼ˆæ”¯æŒè¯´è¯äººæ ‡è®°ï¼‰"""
        print(f"ğŸ“– å¼€å§‹è§£æSRTæ–‡ä»¶: {self.srt_path}")
        
        with open(self.srt_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        print(f"ğŸ“„ æ–‡ä»¶å¤§å°: {len(content)} å­—ç¬¦")
        print(f"ğŸ“„ æ–‡ä»¶å‰200å­—ç¬¦: {repr(content[:200])}")
        
        subtitles = []
        # è§„èŒƒåŒ–æ¢è¡Œç¬¦
        content_normalized = content.replace('\r\n', '\n').replace('\r', '\n')
        
        # å¤„ç†ä¸¤ç§æ ¼å¼ï¼š
        # 1. æ ‡å‡†æ ¼å¼ï¼šè¡Œé—´å•æ¢è¡Œï¼Œå—é—´åŒæ¢è¡Œ
        # 2. éæ ‡å‡€æ ¼å¼ï¼šè¡Œé—´åŒæ¢è¡Œï¼Œå—é—´å¤šä¸ªæ¢è¡Œ
        # å…ˆå°è¯•æ ‡å‡†æ ¼å¼
        blocks = re.split(r'\n\n+', content_normalized.strip())
        
        # å¦‚æœæ‰€æœ‰å—éƒ½åªæœ‰1è¡Œï¼Œè¯´æ˜æ˜¯éæ ‡å‡†æ ¼å¼ï¼Œéœ€è¦é‡æ–°åˆ†ç»„
        if all(len(block.strip().split('\n')) == 1 for block in blocks if block.strip()):
            print("âš ï¸ æ£€æµ‹åˆ°éæ ‡å‡†SRTæ ¼å¼ï¼ˆè¡Œé—´åŒæ¢è¡Œï¼‰ï¼Œé‡æ–°åˆ†ç»„...")
            # æ¯3è¡Œä¸ºä¸€ç»„ï¼ˆåºå·ã€æ—¶é—´ã€æ–‡æœ¬ï¼‰
            lines = [line for line in content_normalized.strip().split('\n') if line.strip()]
            blocks = []
            for i in range(0, len(lines), 3):
                if i + 2 < len(lines):
                    blocks.append('\n'.join(lines[i:i+3]))
                elif i < len(lines):
                    # å¤„ç†æœ€åä¸å®Œæ•´çš„å—
                    blocks.append('\n'.join(lines[i:]))
        
        print(f"ğŸ“¦ åˆ†å‰²åçš„å—æ•°: {len(blocks)}")
        
        for i, block in enumerate(blocks):
            block = block.strip()
            if not block:
                continue
                
            lines = block.split('\n')
            print(f"ğŸ” å— {i+1}: {len(lines)} è¡Œ - {lines[0] if lines else 'empty'}")
            
            if len(lines) >= 3:
                # è§£ææ—¶é—´è½´
                time_match = re.match(r'(\d{2}:\d{2}:\d{2},\d{3})\s*-->\s*(\d{2}:\d{2}:\d{2},\d{3})', lines[1])
                if time_match:
                    text_content = ' '.join(lines[2:])
                    
                    # æå–è¯´è¯äººä¿¡æ¯ï¼ˆæ”¯æŒå¤šç§æ ¼å¼ï¼‰
                    speaker = None
                    clean_text = text_content
                    
                    # åŒ¹é… [spkXX] æ ¼å¼ï¼ˆå¤šè§’è‰²é…éŸ³ï¼‰
                    speaker_match = re.match(r'\[(spk\d+)\]\s*(.*)', text_content)
                    if speaker_match:
                        speaker = speaker_match.group(1)
                        clean_text = speaker_match.group(2)
                    else:
                        # åŒ¹é… [SPEAKER_XX] æ ¼å¼
                        speaker_match = re.match(r'\[(SPEAKER_\d+)\]\s*(.*)', text_content)
                        if speaker_match:
                            speaker = speaker_match.group(1)
                            clean_text = speaker_match.group(2)
                        else:
                            # åŒ¹é… spkXX: æ ¼å¼
                            speaker_match = re.match(r'(spk\d+):\s*(.*)', text_content)
                            if speaker_match:
                                speaker = speaker_match.group(1)
                                clean_text = speaker_match.group(2)
                            else:
                                # åŒ¹é… SPEAKER_XX: æ ¼å¼
                                speaker_match = re.match(r'(SPEAKER_\d+):\s*(.*)', text_content)
                                if speaker_match:
                                    speaker = speaker_match.group(1)
                                    clean_text = speaker_match.group(2)
                    
                    subtitle = {
                        'index': int(lines[0]),
                        'start': time_match.group(1),
                        'end': time_match.group(2),
                        'text': clean_text.strip(),
                        'speaker': speaker  # æ·»åŠ è¯´è¯äººä¿¡æ¯
                    }
                    subtitles.append(subtitle)
                    
                    if speaker:
                        print(f"âœ… è§£æå­—å¹• {subtitle['index']} [{speaker}]: {clean_text[:30]}...")
                    else:
                        print(f"âœ… è§£æå­—å¹• {subtitle['index']}: {clean_text[:30]}...")
                else:
                    print(f"âš ï¸ å— {i+1} æ—¶é—´æ ¼å¼ä¸åŒ¹é…: {lines[1] if len(lines) > 1 else 'N/A'}")
            else:
                print(f"âš ï¸ å— {i+1} è¡Œæ•°ä¸è¶³ ({len(lines)} < 3)")
        
        print(f"âœ… è§£æSRTæ–‡ä»¶æˆåŠŸï¼Œå…± {len(subtitles)} æ¡å­—å¹•")
        return subtitles
    
    def time_to_ms(self, time_str):
        """å°†SRTæ—¶é—´æ ¼å¼è½¬æ¢ä¸ºæ¯«ç§’"""
        # æ ¼å¼: 00:00:05,500
        h, m, s_ms = time_str.split(':')
        s, ms = s_ms.split(',')
        total_ms = int(h) * 3600000 + int(m) * 60000 + int(s) * 1000 + int(ms)
        return total_ms
    
    def synthesize_speech(self, text, index, speaker=None):
        """
        è°ƒç”¨TTS APIåˆæˆè¯­éŸ³
        
        Args:
            text: è¦åˆæˆçš„æ–‡æœ¬
            index: å­—å¹•ç´¢å¼•
            speaker: è¯´è¯äººæ ‡è¯†ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            éŸ³é¢‘æ–‡ä»¶è·¯å¾„
        """
        output_path = self.temp_dir / f"audio_{index:04d}.wav"
        
        try:
            if self.engine == 'gpt-sovits':
                return self._synthesize_gpt_sovits(text, output_path, speaker)
            elif self.engine == 'qwen-tts':
                return self._synthesize_qwen_tts(text, output_path, speaker)
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„TTSå¼•æ“: {self.engine}")
        except Exception as e:
            print(f"âŒ åˆæˆè¯­éŸ³å¤±è´¥ (å­—å¹•{index}): {e}")
            raise
    
    def _synthesize_gpt_sovits(self, text, output_path, speaker=None):
        """ä½¿ç”¨GPT-SoVITSåˆæˆè¯­éŸ³ï¼ˆæ”¯æŒå¤šè§’è‰²ï¼‰"""
        # ç¡®ä¿APIåœ°å€æ­£ç¡®
        api_url = self.api_url
        if not api_url.endswith('/tts'):
            api_url += '/tts'
        
        # æ ¹æ®è¯´è¯äººé€‰æ‹©è§’è‰²é…ç½®
        if speaker and isinstance(self.role_data, dict) and speaker in self.role_data:
            # å¤šè§’è‰²æ¨¡å¼ï¼šä»role_dataä¸­è·å–å¯¹åº”è§’è‰²çš„é…ç½®
            role_config = self.role_data[speaker]
            print(f"ğŸ­ ä½¿ç”¨è§’è‰²é…ç½®: {speaker}")
        elif isinstance(self.role_data, dict) and 'refAudioPath' in self.role_data:
            # å•è§’è‰²æ¨¡å¼ï¼šç›´æ¥ä½¿ç”¨role_data
            role_config = self.role_data
        else:
            # å…œåº•ï¼šä½¿ç”¨é»˜è®¤é…ç½®
            role_config = self.role_data.get('default', {}) if isinstance(self.role_data, dict) else {}
            print(f"âš ï¸ æœªæ‰¾åˆ°è§’è‰² {speaker} çš„é…ç½®ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
        
        # è·å–è¯¥è§’è‰²çš„è¯­é€Ÿç³»æ•°ï¼ˆä¼˜å…ˆä½¿ç”¨è§’è‰²é…ç½®ï¼Œå¦åˆ™ä½¿ç”¨å…¨å±€é…ç½®ï¼‰
        role_speed_factor = role_config.get('speed_factor', self.speed_factor)
        
        # æ„å»ºè¯·æ±‚å‚æ•°
        params = {
            'text': text,
            'text_lang': role_config.get('text_lang', self.text_lang),
            'ref_audio_path': role_config.get('refAudioPath', ''),
            'prompt_text': role_config.get('promptText', ''),
            'prompt_lang': role_config.get('promptLang', 'zh'),
            'speed_factor': role_speed_factor  # ä½¿ç”¨è§’è‰²ç‰¹å®šçš„è¯­é€Ÿ
        }
        
        if speaker:
            print(f"ğŸ”„ è°ƒç”¨GPT-SoVITS API [{speaker}, è¯­é€Ÿ={role_speed_factor}]: {text[:30]}...")
        else:
            print(f"ğŸ”„ è°ƒç”¨GPT-SoVITS API [è¯­é€Ÿ={role_speed_factor}]: {text[:30]}...")
        
        # å‘é€è¯·æ±‚
        response = requests.get(api_url, params=params, timeout=60)
        response.raise_for_status()
        
        # ä¿å­˜éŸ³é¢‘
        with open(output_path, 'wb') as f:
            f.write(response.content)
        
        print(f"âœ… è¯­éŸ³åˆæˆæˆåŠŸ: {output_path.name}")
        return str(output_path)
    
    def _synthesize_qwen_tts(self, text, output_path, speaker=None):
        """ä½¿ç”¨QwenTTSåˆæˆè¯­éŸ³"""
        # TODO: å®ç°QwenTTS APIè°ƒç”¨
        raise NotImplementedError("QwenTTSæš‚æœªå®ç°")
    
    def create_silence(self, duration_ms):
        """åˆ›å»ºé™éŸ³éŸ³é¢‘"""
        return AudioSegment.silent(duration=duration_ms)
    
    def update_progress(self, current, total, subtitle_data=None):
        """æ›´æ–°ä»»åŠ¡è¿›åº¦"""
        if self.task_dict and self.task_id:
            progress = int((current / total) * 100)
            self.task_dict[self.task_id]["progress"] = progress
            if subtitle_data:
                self.task_dict[self.task_id]["current_subtitle"] = subtitle_data
    
    def process(self):
        """
        å¤„ç†å®Œæ•´çš„é…éŸ³æµç¨‹ï¼ˆæ”¯æŒåŒé‡å˜é€Ÿæœºåˆ¶ï¼‰
        
        Returns:
            dict: {
                'audio_path': str,  # æœ€ç»ˆéŸ³é¢‘æ–‡ä»¶è·¯å¾„
                'srt_path': str or None  # æ›´æ–°åçš„SRTæ–‡ä»¶è·¯å¾„ï¼ˆå¦‚æœæœ‰ï¼‰
            }
        """
        print("ğŸ¬ å¼€å§‹TTSé…éŸ³å¤„ç†...")
        
        # 1. è§£æSRTæ–‡ä»¶
        subtitles = self.parse_srt()
        total_subtitles = len(subtitles)
        
        if total_subtitles == 0:
            raise ValueError("SRTæ–‡ä»¶ä¸­æ²¡æœ‰å­—å¹•")
        
        # 2. åˆæˆæ¯æ¡å­—å¹•çš„è¯­éŸ³
        audio_files = []
        subtitle_data = []
        
        for i, subtitle in enumerate(subtitles):
            # æ›´æ–°è¿›åº¦
            self.update_progress(i, total_subtitles, subtitle)
            
            print(f"\nğŸ“ å¤„ç†å­—å¹• {i+1}/{total_subtitles}: {subtitle['text'][:50]}...")
            
            # è·å–æ—¶é—´ä¿¡æ¯
            start_ms = self.time_to_ms(subtitle['start'])
            end_ms = self.time_to_ms(subtitle['end'])
            
            # åˆæˆè¯­éŸ³ï¼ˆä¼ é€’è¯´è¯äººä¿¡æ¯ï¼‰
            speaker = subtitle.get('speaker', None)
            audio_path = self.synthesize_speech(subtitle['text'], i + 1, speaker)
            audio_files.append(audio_path)
            
            # æ„å»ºå­—å¹•æ•°æ®ï¼ˆç”¨äºåŒé‡å˜é€Ÿï¼‰
            subtitle_data.append({
                'start_ms': start_ms,
                'end_ms': end_ms,
                'text': subtitle['text'],
                'audio_file': audio_path,
                'speaker': speaker
            })
        
        # 3. åˆ¤æ–­æ˜¯å¦ä½¿ç”¨ä¿æŒæ€»æ—¶é•¿åŠŸèƒ½ï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼‰
        print(f"\nğŸ” è°ƒè¯•ä¿¡æ¯:")
        print(f"   preserve_total_time = {self.preserve_total_time}")
        print(f"   enable_smart_speedup = {self.enable_smart_speedup}")
        print(f"   auto_align = {self.auto_align}")
        
        if self.preserve_total_time:
            print("\nğŸš€ å¯ç”¨ä¿æŒSRTæ€»æ—¶é•¿ä¸å˜åŠŸèƒ½...")
            
            # ä½¿ç”¨TimelineAdjusteråŠ¨æ€è°ƒæ•´æ—¶é—´è½´
            if True:
                print("\n" + "â±ï¸ "*30)
                print("â±ï¸  ä½¿ç”¨åŠ¨æ€æ—¶é—´è½´è°ƒæ•´ï¼ˆä¿æŒæ€»æ—¶é•¿ï¼‰")
                print(f"ğŸ“Š åŸå§‹SRTæ€»æ—¶é•¿: {subtitle_data[-1]['end_ms']}ms")
                print(f"ğŸ“Š å­—å¹•æ•°é‡: {len(subtitle_data)}")
                print(f"ğŸ“Š é…éŸ³æ–‡ä»¶æ•°é‡: {len(audio_files)}")
                print(f"ğŸ“Š è¯­é€Ÿç³»æ•°: {self.speed_factor}")
                print("â±ï¸ "*30 + "\n")
                
                # ä½¿ç”¨TimelineAdjusteråŠ¨æ€è°ƒæ•´æ—¶é—´è½´
                timeline_adjuster = TimelineAdjuster(
                    subtitles=subtitle_data,
                    audio_files=audio_files,
                    preserve_total_time=True
                )
                
                # è°ƒæ•´æ—¶é—´è½´
                updated_subtitles = timeline_adjuster.adjust_timeline()
                
                # è¾“å‡ºè°ƒæ•´ç»“æœ
                if updated_subtitles:
                    final_time = updated_subtitles[-1]['end_ms']
                    original_time = subtitle_data[-1]['end_ms']
                    print(f"\nğŸ“Š è°ƒæ•´ç»“æœ:")
                    print(f"   åŸå§‹æ€»æ—¶é•¿: {original_time}ms")
                    print(f"   è°ƒæ•´åæ€»æ—¶é•¿: {final_time}ms")
                    print(f"   æ—¶é•¿å·®å¼‚: {final_time - original_time:+d}ms")
                    if abs(final_time - original_time) < 100:
                        print(f"   âœ… æ€»æ—¶é•¿ä¿æŒä¸€è‡´ï¼ˆè¯¯å·® < 0.1ç§’ï¼‰")
                    else:
                        print(f"   âš ï¸ æ€»æ—¶é•¿æœ‰å·®å¼‚ï¼ˆè¯¯å·® = {abs(final_time - original_time)}msï¼‰")
                
                # æ ¹æ®æ›´æ–°åçš„æ—¶é—´è½´åˆå¹¶éŸ³é¢‘
                output_path = self._merge_audio_with_timeline(updated_subtitles, audio_files)
                
                # ä¿å­˜æ›´æ–°åçš„å­—å¹•
                updated_srt_path = self._save_updated_srt(updated_subtitles)
        
        # 4. åˆ¤æ–­æ˜¯å¦ä½¿ç”¨æ™ºèƒ½åŒé‡å˜é€Ÿæœºåˆ¶ï¼ˆä¸ä¿æŒæ€»æ—¶é•¿ï¼‰
        elif self.enable_smart_speedup:
            print("\nğŸš€ å¯ç”¨æ™ºèƒ½åŒé‡å˜é€Ÿæœºåˆ¶...")
            
            if False:  # è¿™ä¸ªåˆ†æ”¯å·²ç»è¢«ä¸Šé¢çš„preserve_total_timeå¤„ç†äº†
                pass
            else:
                print("âš¡ ä½¿ç”¨ä¼ ç»ŸåŒé‡å˜é€Ÿæœºåˆ¶ï¼ˆä¸ä¿æŒæ€»æ—¶é•¿ï¼‰")
                
                # è®¡ç®—åŸå§‹è§†é¢‘æ€»æ—¶é•¿
                raw_total_time_ms = subtitle_data[-1]['end_ms'] if subtitle_data else 0
                
                # åˆ›å»ºåŒé‡å˜é€Ÿè°ƒæ•´å™¨
                adjuster = SpeedRateAdjuster(
                    subtitles=subtitle_data,
                    audio_files=audio_files,
                    output_dir=str(self.output_dir),
                    enable_audio_speedup=self.enable_audio_speedup,
                    enable_video_slowdown=self.enable_video_slowdown,
                    max_audio_speed_rate=self.max_audio_speed_rate,
                    max_video_pts_rate=self.max_video_pts_rate,
                    remove_silent_gaps=self.remove_silent_gaps,
                    align_subtitle_audio=self.auto_align,
                    raw_total_time_ms=raw_total_time_ms
                )
                
                # æ‰§è¡ŒåŒé‡å˜é€Ÿå¤„ç†
                output_path, updated_subtitles = adjuster.process()
                
                # ä¿å­˜æ›´æ–°åçš„å­—å¹•
                updated_srt_path = self._save_updated_srt(updated_subtitles)
        
        # 5. ä½¿ç”¨ä¼ ç»Ÿæ–¹å¼æ‹¼æ¥éŸ³é¢‘
        else:
            print("\nğŸ”— ä½¿ç”¨ä¼ ç»Ÿæ–¹å¼æ‹¼æ¥éŸ³é¢‘...")
            audio_segments = []
            last_end_time = 0
            
            for i, subtitle_info in enumerate(subtitle_data):
                start_ms = subtitle_info['start_ms']
                end_ms = subtitle_info['end_ms']
                duration_ms = end_ms - start_ms
                
                # å¦‚æœéœ€è¦ï¼Œæ·»åŠ é™éŸ³ä»¥å¯¹é½æ—¶é—´è½´
                if start_ms > last_end_time:
                    silence_duration = start_ms - last_end_time
                    print(f"  â¸ï¸  æ·»åŠ é™éŸ³: {silence_duration}ms")
                    audio_segments.append(self.create_silence(silence_duration))
                
                # åŠ è½½éŸ³é¢‘
                audio = AudioSegment.from_wav(subtitle_info['audio_file'])
                
                # å¦‚æœå¯ç”¨è‡ªåŠ¨å¯¹é½ï¼Œè°ƒæ•´éŸ³é¢‘é•¿åº¦ä»¥åŒ¹é…å­—å¹•æ—¶é•¿
                if self.auto_align:
                    audio_duration = len(audio)
                    if audio_duration > duration_ms:
                        # éŸ³é¢‘å¤ªé•¿ï¼ŒåŠ é€Ÿ
                        speed_ratio = audio_duration / duration_ms
                        print(f"  âš¡ åŠ é€ŸéŸ³é¢‘: {speed_ratio:.2f}x")
                        audio = audio.speedup(playback_speed=speed_ratio)
                    elif audio_duration < duration_ms:
                        # éŸ³é¢‘å¤ªçŸ­ï¼Œæ·»åŠ é™éŸ³
                        padding = duration_ms - audio_duration
                        print(f"  â¸ï¸  æ·»åŠ å°¾éƒ¨é™éŸ³: {padding}ms")
                        audio = audio + self.create_silence(padding)
                
                audio_segments.append(audio)
                last_end_time = end_ms
                
                # æ·»åŠ å­—å¹•é—´éš”é™éŸ³
                if i < total_subtitles - 1:
                    silence_ms = int(self.silence_duration * 1000)
                    audio_segments.append(self.create_silence(silence_ms))
                    last_end_time += silence_ms
            
            # æ‹¼æ¥æ‰€æœ‰éŸ³é¢‘
            final_audio = sum(audio_segments)
            
            # å¯¼å‡ºæœ€ç»ˆéŸ³é¢‘
            output_path = self.output_dir / "dubbing_result.wav"
            print(f"ğŸ’¾ å¯¼å‡ºæœ€ç»ˆéŸ³é¢‘: {output_path}")
            final_audio.export(output_path, format="wav")
            output_path = str(output_path)
            updated_srt_path = None  # ä¼ ç»Ÿæ–¹å¼ä¸ç”Ÿæˆæ›´æ–°åçš„SRT
        
        # 6. æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        print("ğŸ§¹ æ¸…ç†ä¸´æ—¶æ–‡ä»¶...")
        for temp_file in self.temp_dir.glob("*.wav"):
            temp_file.unlink()
        
        print(f"âœ… TTSé…éŸ³å®Œæˆï¼è¾“å‡ºæ–‡ä»¶: {output_path}")
        if updated_srt_path:
            print(f"âœ… æ›´æ–°åçš„å­—å¹•: {updated_srt_path}")
        
        return {
            'audio_path': output_path,
            'srt_path': updated_srt_path
        }
    
    def _speedup_audio_ffmpeg(self, input_file, output_file, speed_ratio, target_duration_ms):
        """
        ä½¿ç”¨FFmpegé«˜è´¨é‡åŠ é€ŸéŸ³é¢‘
        
        Args:
            input_file: è¾“å…¥éŸ³é¢‘æ–‡ä»¶
            output_file: è¾“å‡ºéŸ³é¢‘æ–‡ä»¶
            speed_ratio: åŠ é€Ÿå€ç‡
            target_duration_ms: ç›®æ ‡æ—¶é•¿ï¼ˆæ¯«ç§’ï¼‰
        
        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        try:
            import subprocess
            
            target_duration_sec = target_duration_ms / 1000.0
            
            # å°è¯•ä½¿ç”¨ rubberband æ»¤é•œï¼ˆé«˜è´¨é‡ï¼‰
            cmd = [
                'ffmpeg', '-y', '-i', input_file,
                '-filter:a', f'rubberband=tempo={speed_ratio}',
                '-t', f'{target_duration_sec:.4f}',
                '-ar', '44100',
                '-ac', '2',
                '-c:a', 'pcm_s16le',
                output_file
            ]
            
            result = subprocess.run(cmd, capture_output=True, text=True, encoding='utf-8', errors='ignore')
            
            if result.returncode != 0:
                # rubberband ä¸å¯ç”¨ï¼Œä½¿ç”¨ atempo
                print(f"    âš ï¸ rubberband ä¸å¯ç”¨ï¼Œä½¿ç”¨ atempo")
                
                # atempo é™åˆ¶åœ¨ 0.5-2.0 ä¹‹é—´ï¼Œéœ€è¦é“¾å¼å¤„ç†
                tempo_filters = []
                current_tempo = speed_ratio
                while current_tempo > 2.0:
                    tempo_filters.append("atempo=2.0")
                    current_tempo /= 2.0
                while current_tempo < 0.5:
                    tempo_filters.append("atempo=0.5")
                    current_tempo /= 0.5
                if 0.5 <= current_tempo <= 2.0:
                    tempo_filters.append(f"atempo={current_tempo}")
                
                filter_str = ",".join(tempo_filters)
                
                cmd = [
                    'ffmpeg', '-y', '-i', input_file,
                    '-filter:a', filter_str,
                    '-t', f'{target_duration_sec:.4f}',
                    '-ar', '44100',
                    '-ac', '2',
                    '-c:a', 'pcm_s16le',
                    output_file
                ]
                
                result = subprocess.run(cmd, capture_output=True, text=True, encoding='utf-8', errors='ignore')
                
                if result.returncode != 0:
                    print(f"    âŒ FFmpeg åŠ é€Ÿå¤±è´¥: {result.stderr}")
                    return False
            
            return True
            
        except Exception as e:
            print(f"    âŒ éŸ³é¢‘åŠ é€Ÿå¼‚å¸¸: {e}")
            return False
    
    def _merge_audio_with_timeline(self, updated_subtitles, audio_files):
        """
        æ ¹æ®æ›´æ–°åçš„æ—¶é—´è½´åˆå¹¶éŸ³é¢‘
        
        Args:
            updated_subtitles: æ›´æ–°åçš„å­—å¹•åˆ—è¡¨ï¼ˆåŒ…å«æ–°çš„start_mså’Œend_msï¼‰
            audio_files: é…éŸ³æ–‡ä»¶åˆ—è¡¨
            
        Returns:
            æœ€ç»ˆéŸ³é¢‘æ–‡ä»¶è·¯å¾„
        """
        print("\nğŸ”— æ ¹æ®åŠ¨æ€æ—¶é—´è½´åˆå¹¶éŸ³é¢‘...")
        
        audio_segments = []
        current_time = 0
        
        # åˆ›å»ºä¸´æ—¶ç›®å½•ç”¨äºå­˜æ”¾åŠ é€Ÿåçš„éŸ³é¢‘
        speedup_temp_dir = self.temp_dir / "speedup"
        speedup_temp_dir.mkdir(parents=True, exist_ok=True)
        
        for i, subtitle in enumerate(updated_subtitles):
            # æ·»åŠ å­—å¹•å‰çš„é™éŸ³é—´éš™
            if subtitle['start_ms'] > current_time:
                gap = subtitle['start_ms'] - current_time
                print(f"  å­—å¹• {i+1} å‰æ·»åŠ é™éŸ³: {gap}ms")
                audio_segments.append(AudioSegment.silent(duration=gap))
                current_time += gap
            
            # åŠ è½½é…éŸ³éŸ³é¢‘
            audio_file = audio_files[i] if i < len(audio_files) else None
            if audio_file and os.path.exists(audio_file):
                try:
                    audio = AudioSegment.from_file(audio_file)
                    audio_duration = len(audio)
                    
                    # è®¡ç®—ç›®æ ‡æ—¶é•¿
                    target_duration = subtitle['end_ms'] - subtitle['start_ms']
                    
                    # æ£€æŸ¥æ˜¯å¦éœ€è¦åŠ é€Ÿï¼ˆä½¿ç”¨ original_duration_ms å’Œ adjusted_duration_msï¼‰
                    original_duration = subtitle.get('original_duration_ms', audio_duration)
                    adjusted_duration = subtitle.get('adjusted_duration_ms', target_duration)
                    
                    # å¦‚æœè°ƒæ•´åæ—¶é•¿ < åŸå§‹æ—¶é•¿ï¼Œè¯´æ˜éœ€è¦åŠ é€Ÿ
                    if original_duration > adjusted_duration and abs(original_duration - adjusted_duration) > 10:
                        speed_ratio = original_duration / adjusted_duration
                        print(f"  å­—å¹• {i+1}: åŠ é€ŸéŸ³é¢‘ {speed_ratio:.2f}x ({original_duration}ms -> {adjusted_duration}ms)")
                        
                        # ä½¿ç”¨FFmpegåŠ é€Ÿ
                        speedup_output = speedup_temp_dir / f"speedup_{i:04d}.wav"
                        if self._speedup_audio_ffmpeg(audio_file, str(speedup_output), speed_ratio, adjusted_duration):
                            # åŠ é€ŸæˆåŠŸï¼ŒåŠ è½½åŠ é€Ÿåçš„éŸ³é¢‘
                            audio = AudioSegment.from_file(str(speedup_output))
                            print(f"    âœ… åŠ é€ŸæˆåŠŸï¼Œå®é™…æ—¶é•¿: {len(audio)}ms")
                        else:
                            # åŠ é€Ÿå¤±è´¥ï¼Œä½¿ç”¨pydubçš„speedupä½œä¸ºå¤‡é€‰
                            print(f"    âš ï¸ FFmpegåŠ é€Ÿå¤±è´¥ï¼Œä½¿ç”¨pydubå¤‡é€‰æ–¹æ¡ˆ")
                            audio = audio.speedup(playback_speed=speed_ratio)
                    
                    # ç¡®ä¿éŸ³é¢‘æ—¶é•¿åŒ¹é…ç›®æ ‡æ—¶é•¿
                    actual_audio_duration = len(audio)
                    if abs(actual_audio_duration - target_duration) > 10:
                        if actual_audio_duration > target_duration:
                            # éŸ³é¢‘ä»ç„¶å¤ªé•¿ï¼Œæˆªæ–­
                            audio = audio[:target_duration]
                            print(f"    âš ï¸ éŸ³é¢‘ä»ç„¶å¤ªé•¿ï¼Œæˆªæ–­åˆ° {target_duration}ms")
                        else:
                            # éŸ³é¢‘å¤ªçŸ­ï¼Œæ·»åŠ å°¾éƒ¨é™éŸ³
                            padding = target_duration - actual_audio_duration
                            audio = audio + AudioSegment.silent(duration=padding)
                            print(f"    âš ï¸ éŸ³é¢‘å¤ªçŸ­ï¼Œæ·»åŠ å°¾éƒ¨é™éŸ³ {padding}ms")
                    
                    audio_segments.append(audio)
                    current_time += len(audio)
                    print(f"  å­—å¹• {i+1}: æ·»åŠ é…éŸ³ {len(audio)}ms")
                    
                except Exception as e:
                    print(f"  âš ï¸ å­—å¹• {i+1} åŠ è½½éŸ³é¢‘å¤±è´¥: {e}ï¼Œä½¿ç”¨é™éŸ³")
                    silence_duration = subtitle['end_ms'] - subtitle['start_ms']
                    audio_segments.append(AudioSegment.silent(duration=silence_duration))
                    current_time += silence_duration
            else:
                # ä½¿ç”¨é™éŸ³å¡«å……
                silence_duration = subtitle['end_ms'] - subtitle['start_ms']
                print(f"  å­—å¹• {i+1}: ä½¿ç”¨é™éŸ³å¡«å…… {silence_duration}ms")
                audio_segments.append(AudioSegment.silent(duration=silence_duration))
                current_time += silence_duration
        
        # åˆå¹¶æ‰€æœ‰éŸ³é¢‘ç‰‡æ®µ
        print(f"\n  ğŸ”— åˆå¹¶ {len(audio_segments)} ä¸ªéŸ³é¢‘ç‰‡æ®µ...")
        final_audio = sum(audio_segments)
        
        # å¯¼å‡ºæœ€ç»ˆéŸ³é¢‘
        output_path = self.output_dir / "dubbing_result.wav"
        print(f"  ğŸ’¾ å¯¼å‡ºæœ€ç»ˆéŸ³é¢‘: {output_path}")
        final_audio.export(str(output_path), format="wav")
        
        print(f"  âœ… æœ€ç»ˆéŸ³é¢‘æ—¶é•¿: {len(final_audio)}ms ({len(final_audio)/1000:.1f}ç§’)")
        
        return str(output_path)
    
    def _save_updated_srt(self, subtitles):
        """
        ä¿å­˜æ›´æ–°åçš„å­—å¹•æ–‡ä»¶
        
        Returns:
            str: ä¿å­˜çš„SRTæ–‡ä»¶è·¯å¾„
        """
        output_srt = self.output_dir / "updated_subtitles.srt"
        
        with open(output_srt, 'w', encoding='utf-8') as f:
            for i, subtitle in enumerate(subtitles):
                f.write(f"{i+1}\n")
                
                # è½¬æ¢æ¯«ç§’ä¸ºSRTæ—¶é—´æ ¼å¼
                start_time = self._ms_to_srt_time(subtitle['start_ms'])
                end_time = self._ms_to_srt_time(subtitle['end_ms'])
                
                f.write(f"{start_time} --> {end_time}\n")
                f.write(f"{subtitle['text']}\n\n")
        
        print(f"ğŸ’¾ ä¿å­˜æ›´æ–°åçš„å­—å¹•: {output_srt}")
        return str(output_srt)
    
    def _ms_to_srt_time(self, ms):
        """å°†æ¯«ç§’è½¬æ¢ä¸ºSRTæ—¶é—´æ ¼å¼"""
        hours = int(ms // 3600000)
        minutes = int((ms % 3600000) // 60000)
        seconds = int((ms % 60000) // 1000)
        milliseconds = int(ms % 1000)
        return f"{hours:02d}:{minutes:02d}:{seconds:02d},{milliseconds:03d}"


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    processor = TTSDubbingProcessor(
        srt_path="test.srt",
        output_dir="output",
        engine="gpt-sovits",
        role_data={
            "refAudioPath": "cs3.mp3",
            "promptText": "æµ‹è¯•æ–‡æœ¬",
            "promptLang": "zh"
        },
        api_url="http://192.168.110.204:9880"
    )
    
    result = processor.process()
    print(f"ç»“æœ: {result}")
