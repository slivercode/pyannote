# 多角色TTS配音功能使用说明

## 功能概述

多角色TTS配音功能允许您通过导入带有说话人标识的SRT字幕文件，自动为不同的说话人分配不同的TTS配音角色，实现多角色智能配音。

## 核心特性

- ✅ **自动识别说话人**：从SRT文件中自动提取说话人标识
- ✅ **灵活角色映射**：为每个说话人独立配置TTS角色
- ✅ **实时进度显示**：可视化配音进度和当前处理状态
- ✅ **自动时间对齐**：支持音频长度自动调整以匹配字幕时间轴
- ✅ **多引擎支持**：支持GPT-SoVITS和QwenTTS引擎

## 使用流程

### 1. 准备带说话人标识的SRT文件

SRT文件格式要求：
```srt
1
00:00:00,000 --> 00:00:03,500
[spk00] 大家好，欢迎收看今天的节目

2
00:00:04,000 --> 00:00:08,000
[spk01] 今天我们要讲解的是人工智能技术

3
00:00:08,500 --> 00:00:12,000
[spk00] 人工智能正在改变我们的生活
```

**说话人标识格式**：`[说话人名称] 文本内容`
- 说话人名称可以是任意字符串，如 `spk00`、`spk01`、`角色A`、`旁白` 等
- 方括号 `[]` 必须紧跟在时间轴之后的文本行开头

### 2. 配置TTS引擎和角色

#### 步骤1：打开TTS模型管理
1. 点击页面右上角的 **"TTS模型管理"** 按钮
2. 配置您要使用的TTS引擎（GPT-SoVITS 或 QwenTTS）

#### 步骤2：添加配音角色
以GPT-SoVITS为例：
1. 启用GPT-SoVITS引擎
2. 填写API地址（如 `http://127.0.0.1:9880`）
3. 点击 **"添加角色"**
4. 为每个角色配置：
   - **角色名称**：如"男主角"、"女主角"
   - **参考音频路径**：参考音频文件的路径
   - **提示文本**：参考音频对应的文本
   - **提示语言**：参考音频的语言
   - **语速系数**：调整语速（0.5-2.0）

5. 点击 **"保存配置"**

### 3. 开始多角色配音

#### 步骤1：切换到多角色配音模式
1. 进入 **"TTS配音"** 标签页
2. 点击 **"多角色配音"** 子标签

#### 步骤2：上传SRT文件
1. 点击 **"带说话人标识的SRT文件"** 上传按钮
2. 选择您准备好的SRT文件
3. 系统会自动检测并显示所有说话人

#### 步骤3：选择TTS引擎
在下拉框中选择您配置好的TTS引擎（GPT-SoVITS 或 QwenTTS）

#### 步骤4：配置角色映射
为每个检测到的说话人分配对应的TTS角色：
- 系统会显示每个说话人及其字幕数量
- 在下拉框中为每个说话人选择一个配音角色
- 确保所有说话人都已分配角色

#### 步骤5：调整高级选项（可选）
- **合成语言**：选择中文、英文、日文或韩文
- **语速调整**：0.5x - 2.0x
- **静音间隔**：字幕之间的静音时长（秒）
- **自动对齐时间轴**：自动调整音频长度以匹配字幕时间

#### 步骤6：开始配音
点击 **"开始多角色配音"** 按钮，系统将：
1. 解析SRT文件
2. 验证角色配置
3. 逐条合成字幕语音
4. 拼接所有音频片段
5. 导出最终配音文件

### 4. 查看结果

配音完成后，您可以：
- **在线播放**：直接在浏览器中播放生成的音频
- **下载音频**：点击"下载音频"按钮保存到本地
- **打开文件夹**：查看输出目录中的所有文件

## 输出文件说明

配音完成后，输出目录包含：
- `multi_role_dubbing_result.wav` - 最终配音音频文件
- `role_stats.json` - 角色统计信息（字幕数量、说话人列表等）
- `subtitles_with_speakers.srt` - 上传的原始SRT文件备份
- `roles_config.json` - 角色配置备份
- `temp_audio/` - 临时音频文件（配音完成后自动清理）

## 与OCR说话人分配功能的集成

本功能可以与 **"OCR字幕说话人分配"** 功能无缝衔接：

1. 使用 **OCR字幕说话人分配** 功能处理视频，生成带说话人标识的SRT文件
2. 下载生成的 `字幕说话人分配结果.srt` 文件
3. 在 **多角色TTS配音** 中上传该文件
4. 系统自动识别说话人（如 spk00、spk01 等）
5. 为每个说话人分配TTS角色
6. 一键生成多角色配音

## 技术架构

### 后端处理流程
```
1. 接收SRT文件和角色配置
2. 解析SRT，提取说话人和文本
3. 验证角色配置完整性
4. 逐条合成语音：
   - 根据说话人选择对应的TTS配置
   - 调用TTS API生成音频
   - 根据时间轴添加静音
   - 自动对齐音频长度
5. 拼接所有音频片段
6. 导出最终音频文件
7. 生成统计信息
```

### 核心文件

#### 后端
- `src/scripts/tts_multi_role_dubbing.py` - 多角色配音处理器
- `src/main.py` - API接口（`/api/tts-dubbing/multi-role`）

#### 前端
- `src/static/index.html` - UI界面和交互逻辑

## 常见问题

### Q1: 为什么检测不到说话人？
**A**: 请确保SRT文件中的说话人标识格式正确：
- 必须使用方括号 `[]`
- 说话人标识必须在文本行开头
- 示例：`[spk00] 文本内容`

### Q2: 配音失败怎么办？
**A**: 请检查：
1. TTS引擎是否正常运行（如GPT-SoVITS服务）
2. API地址是否正确
3. 角色配置中的参考音频路径是否存在
4. 网络连接是否正常

### Q3: 如何调整某个角色的语速？
**A**: 有两种方式：
1. 在TTS模型管理中，为该角色单独设置语速系数
2. 在配音配置的"高级选项"中调整全局语速

### Q4: 支持哪些TTS引擎？
**A**: 目前支持：
- **GPT-SoVITS**：需要本地部署GPT-SoVITS服务
- **QwenTTS**：需要阿里云API密钥（开发中）

### Q5: 生成的音频可以用于商业用途吗？
**A**: 这取决于：
1. 您使用的TTS引擎的许可协议
2. 参考音频的版权归属
3. 生成内容的用途
请确保遵守相关法律法规和服务条款。

## 最佳实践

1. **角色配置**：
   - 为不同性别、年龄的角色准备不同的参考音频
   - 参考音频质量越高，合成效果越好
   - 提示文本应与参考音频内容完全一致

2. **SRT文件准备**：
   - 确保时间轴准确
   - 字幕长度适中（避免单条字幕过长）
   - 说话人标识统一规范

3. **性能优化**：
   - 字幕数量较多时，配音时间较长，请耐心等待
   - 建议先用少量字幕测试配置是否正确
   - 可以在TTS引擎端优化推理速度

## 更新日志

### v1.0.0 (2025-12-10)
- ✨ 新增多角色TTS配音功能
- ✨ 支持自动识别SRT中的说话人
- ✨ 支持灵活的角色映射配置
- ✨ 支持实时进度显示
- ✨ 支持自动时间对齐
- ✨ 集成GPT-SoVITS引擎

## 技术支持

如有问题或建议，请联系开发团队或提交Issue。
